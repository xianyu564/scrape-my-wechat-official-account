#!/usr/bin/env python3
"""
ä¸­æ–‡è¯­æ–™åˆ†æå‘½ä»¤è¡Œå·¥å…·
"""

import os
import sys
import argparse
import warnings
from pathlib import Path
from typing import List, Optional
import pandas as pd
from tqdm import tqdm

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.io_utils import scan_articles, read_text, get_corpus_stats
from src.tokenize_zh import ChineseTokenizer
from src.freq_stats import (
    term_freq_overall, term_freq_by_year, tfidf_topk_by_year,
    zipf_plot, save_freq_stats, get_stats_summary
)
from src.wordcloud_viz import (
    generate_overall_wordcloud, generate_yearly_wordclouds,
    create_wordcloud_comparison
)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸­æ–‡è¯­æ–™çš„è¯é¢‘åˆ†æã€TF-IDF å…³é”®è¯ä¸è¯äº‘å¯è§†åŒ–",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºç¡€åˆ†æ
  python -m analysis.src.cli --root Wechat-Backup/æ–‡ä¸åŠ ç‚¹çš„å¼ è¡”ç‘œ
  
  # å¸¦æ—¶é—´è¿‡æ»¤
  python -m analysis.src.cli --root Wechat-Backup/æ–‡ä¸åŠ ç‚¹çš„å¼ è¡”ç‘œ --start 2020-01-01 --end 2023-12-31
  
  # æŒ‡å®šå¹´ä»½å’Œå‚æ•°
  python -m analysis.src.cli --root Wechat-Backup/æ–‡ä¸åŠ ç‚¹çš„å¼ è¡”ç‘œ --years 2021,2022,2023 --topk 100
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--root', required=True,
                       help='è¯­æ–™æ ¹ç›®å½•ï¼Œå¦‚ Wechat-Backup/æ–‡ä¸åŠ ç‚¹çš„å¼ è¡”ç‘œ')
    
    # æ—¶é—´è¿‡æ»¤
    parser.add_argument('--start', 
                       help='å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD')
    parser.add_argument('--end',
                       help='ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD') 
    parser.add_argument('--years',
                       help='å¹´ä»½ç™½åå•ï¼Œé€—å·åˆ†éš”ï¼Œå¦‚ 2017,2018,2019')
    
    # åˆ†è¯å‚æ•°
    parser.add_argument('--min-df', type=int, default=5,
                       help='TF-IDF æœ€å°æ–‡æ¡£é¢‘ç‡ (default: 5)')
    parser.add_argument('--max-df', type=float, default=0.85,
                       help='TF-IDF æœ€å¤§æ–‡æ¡£é¢‘ç‡ (default: 0.85)')
    parser.add_argument('--ngram-max', type=int, default=2,
                       help='æœ€å¤§ n-gram é•¿åº¦ (default: 2)')
    parser.add_argument('--topk', type=int, default=50,
                       help='æ¯å¹´è¿”å›çš„å…³é”®è¯æ•°é‡ (default: 50)')
    
    # å¯é€‰æ–‡ä»¶è·¯å¾„
    parser.add_argument('--font-path',
                       help='ä¸­æ–‡å­—ä½“æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--userdict',
                       help='è‡ªå®šä¹‰è¯å…¸æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--extra-stopwords',
                       help='é¢å¤–åœç”¨è¯æ–‡ä»¶è·¯å¾„')
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument('--make-wordcloud', type=int, default=1,
                       help='æ˜¯å¦ç”Ÿæˆè¯äº‘ (1=æ˜¯, 0=å¦, default: 1)')
    parser.add_argument('--report', type=int, default=1,
                       help='æ˜¯å¦ç”ŸæˆæŠ¥å‘Š (1=æ˜¯, 0=å¦, default: 1)')
    parser.add_argument('--output-dir', default='analysis/out',
                       help='è¾“å‡ºç›®å½• (default: analysis/out)')
    
    return parser.parse_args()


def load_extra_stopwords(file_path: str) -> List[str]:
    """åŠ è½½é¢å¤–åœç”¨è¯"""
    if not os.path.exists(file_path):
        return []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except Exception as e:
        warnings.warn(f"åŠ è½½é¢å¤–åœç”¨è¯å¤±è´¥: {e}")
        return []


def create_tokenizer_func(tokenizer, chinese_only=True, ngram_max=1):
    """åˆ›å»ºåˆ†è¯å‡½æ•°ä¾› TF-IDF ä½¿ç”¨"""
    def tokenize_func(text):
        return tokenizer.tokenize(text, chinese_only, ngram_max)
    return tokenize_func


def generate_report(stats_summary: dict, 
                   freq_overall: pd.DataFrame,
                   freq_by_year: pd.DataFrame, 
                   tfidf_by_year: pd.DataFrame,
                   output_dir: str) -> str:
    """ç”Ÿæˆ Markdown åˆ†ææŠ¥å‘Š"""
    
    report_path = os.path.join(output_dir, "REPORT.md")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ä¸­æ–‡è¯­æ–™åˆ†ææŠ¥å‘Š\n\n")
        
        # æ¦‚è§ˆç»Ÿè®¡
        f.write("## ğŸ“Š è¯­æ–™æ¦‚è§ˆ\n\n")
        if 'total_unique_words' in stats_summary:
            f.write(f"- **æ€»è¯æ±‡æ•°**: {stats_summary['total_unique_words']:,}\n")
            f.write(f"- **æ€»è¯é¢‘**: {stats_summary['total_word_freq']:,}\n")
        
        if 'years' in stats_summary:
            f.write(f"- **å¹´ä»½èŒƒå›´**: {min(stats_summary['years'])} - {max(stats_summary['years'])}\n")
            f.write(f"- **è¦†ç›–å¹´æ•°**: {len(stats_summary['years'])} å¹´\n\n")
        
        # æ•´ä½“è¯é¢‘ Top 50
        f.write("## ğŸ”¥ æ•´ä½“é«˜é¢‘è¯æ±‡ (Top 50)\n\n")
        if not freq_overall.empty:
            top_50 = freq_overall.head(50)
            f.write("| æ’å | è¯æ±‡ | é¢‘æ¬¡ |\n")
            f.write("|------|------|------|\n")
            for i, row in top_50.iterrows():
                f.write(f"| {i+1} | {row['word']} | {row['freq']:,} |\n")
            f.write("\n")
        
        # æ•´ä½“è¯äº‘
        f.write("## ğŸ¨ æ•´ä½“è¯äº‘\n\n")
        f.write("![æ•´ä½“è¯äº‘](wordcloud_overall.png)\n\n")
        
        # å¹´åº¦åˆ†æ
        if 'years' in stats_summary:
            f.write("## ğŸ“… å¹´åº¦è¯é¢‘åˆ†æ\n\n")
            
            for year in sorted(stats_summary['years']):
                f.write(f"### {year} å¹´\n\n")
                
                # å¹´åº¦ Top 20 è¯é¢‘
                year_freq = freq_by_year[freq_by_year['year'] == year].head(20)
                if not year_freq.empty:
                    f.write("**é«˜é¢‘è¯æ±‡ (Top 20)**:\n\n")
                    f.write("| æ’å | è¯æ±‡ | é¢‘æ¬¡ |\n")
                    f.write("|------|------|------|\n")
                    for i, row in year_freq.iterrows():
                        rank = list(year_freq.index).index(i) + 1
                        f.write(f"| {rank} | {row['word']} | {row['freq']:,} |\n")
                    f.write("\n")
                
                # å¹´åº¦ TF-IDF Top 20  
                if not tfidf_by_year.empty and 'year' in tfidf_by_year.columns:
                    year_tfidf = tfidf_by_year[tfidf_by_year['year'] == year].head(20)
                    if not year_tfidf.empty:
                        f.write("**TF-IDF å…³é”®è¯ (Top 20)**:\n\n")
                        f.write("| æ’å | è¯æ±‡ | TF-IDF åˆ†æ•° |\n")
                        f.write("|------|------|-------------|\n")
                        for i, row in year_tfidf.iterrows():
                            rank = list(year_tfidf.index).index(i) + 1
                            f.write(f"| {rank} | {row['word']} | {row['score']:.4f} |\n")
                        f.write("\n")
                
                # å¹´åº¦è¯äº‘
                f.write(f"**{year} å¹´è¯äº‘**:\n\n")
                f.write(f"![{year}å¹´è¯äº‘](wordcloud_{year}.png)\n\n")
        
        # Zipf å®šå¾‹åˆ†æ
        f.write("## ğŸ“ˆ Zipf å®šå¾‹åˆ†æ\n\n")
        f.write("æ ¹æ® Zipf å®šå¾‹ï¼Œè¯é¢‘ä¸å…¶æ’åå‘ˆåæ¯”å…³ç³»ï¼ˆf âˆ 1/r^Î±ï¼‰ã€‚")
        f.write("ç†æƒ³æƒ…å†µä¸‹ï¼Œæ‹Ÿåˆæ–œç‡åº”æ¥è¿‘ -1ã€‚\n\n")
        f.write("![Zipfå®šå¾‹åˆ†æ](zipf_overall.png)\n\n")
        f.write("**è§£è¯»**: è¯­è¨€é£æ ¼ä½“ç°äº†å…¸å‹çš„é•¿å°¾åˆ†å¸ƒç‰¹å¾ï¼Œ")
        f.write("å°‘æ•°é«˜é¢‘è¯æ‰¿è½½ä¸»è¦è¯­è¨€ä¿¡æ¯ï¼Œå¤§é‡ä½é¢‘è¯ä¸°å¯Œè¡¨è¾¾çš„ç»†èŠ‚å’Œä¸ªæ€§ã€‚\n\n")
        
        # å¹´åº¦å£å·ç”Ÿæˆ
        if 'top_tfidf_words' in stats_summary and stats_summary['top_tfidf_words']:
            f.write("## ğŸ¯ å¹´åº¦å…³é”®è¯å£å·\n\n")
            for year, words in stats_summary['top_tfidf_words'].items():
                if words:
                    top_3 = words[:3]
                    slogan = f"åœ¨{year}å¹´ï¼Œæˆ‘ä¸“æ³¨äº{top_3[0]}ï¼Œæ·±å…¥æ¢ç´¢{top_3[1] if len(top_3) > 1 else 'æ–°é¢†åŸŸ'}ï¼ŒæŒç»­æ€è€ƒ{top_3[2] if len(top_3) > 2 else 'äººç”Ÿå“²ç†'}ã€‚"
                    f.write(f"**{year}**: {slogan}\n\n")
        
        # ç”Ÿæˆæ—¶é—´
        from datetime import datetime
        f.write(f"\n---\n\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
    
    print(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    return report_path


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print("ğŸš€ å¼€å§‹ä¸­æ–‡è¯­æ–™åˆ†æ...")
    
    # è§£æå¹´ä»½åˆ—è¡¨
    years = None
    if args.years:
        years = [year.strip() for year in args.years.split(',')]
    
    # æ‰«ææ–‡ç« 
    print(f"ğŸ“ æ‰«æè¯­æ–™ç›®å½•: {args.root}")
    articles = scan_articles(
        root_dir=args.root,
        start_date=args.start,
        end_date=args.end,
        years=years
    )
    
    if not articles:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œè¿‡æ»¤æ¡ä»¶")
        return
    
    # æ˜¾ç¤ºè¯­æ–™ç»Ÿè®¡
    corpus_stats = get_corpus_stats(articles)
    print(f"ğŸ“Š è¯­æ–™ç»Ÿè®¡: {corpus_stats}")
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    print("ğŸ”¤ åˆå§‹åŒ–ä¸­æ–‡åˆ†è¯å™¨...")
    extra_stopwords = []
    if args.extra_stopwords:
        extra_stopwords = load_extra_stopwords(args.extra_stopwords)
    
    tokenizer = ChineseTokenizer(
        userdict_path=args.userdict,
        stopwords_path="analysis/assets/stopwords_zh.txt",
        extra_stopwords=extra_stopwords
    )
    
    # è¯»å–å’Œåˆ†è¯æ–‡æœ¬
    print("ğŸ“– è¯»å–æ–‡ç« å†…å®¹å¹¶åˆ†è¯...")
    corpus_tokens = []
    corpus_by_year = {}
    texts_by_year = {}
    
    for article in tqdm(articles, desc="å¤„ç†æ–‡ç« "):
        text = read_text(article)
        if not text:
            continue
        
        tokens = tokenizer.tokenize(text, chinese_only=True, ngram_max=args.ngram_max)
        if tokens:
            corpus_tokens.append(tokens)
            
            # æŒ‰å¹´åˆ†ç»„
            year = article.year
            if year not in corpus_by_year:
                corpus_by_year[year] = []
                texts_by_year[year] = []
            
            corpus_by_year[year].append(tokens)
            texts_by_year[year].append(text)
    
    if not corpus_tokens:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„åˆ†è¯ç»“æœ")
        return
    
    print(f"âœ… æˆåŠŸå¤„ç† {len(corpus_tokens)} ç¯‡æ–‡ç« ")
    
    # è¯é¢‘ç»Ÿè®¡
    print("ğŸ“Š è®¡ç®—è¯é¢‘ç»Ÿè®¡...")
    freq_overall = term_freq_overall(corpus_tokens)
    freq_by_year = term_freq_by_year(corpus_by_year)
    
    # TF-IDF åˆ†æ
    print("ğŸ” è®¡ç®— TF-IDF å…³é”®è¯...")
    tokenizer_func = create_tokenizer_func(tokenizer, chinese_only=True, ngram_max=args.ngram_max)
    
    # ç®€åŒ–çš„ TF-IDF åˆ†æ - å¦‚æœå¤±è´¥å°±åˆ›å»ºç©ºçš„ DataFrame
    try:
        tfidf_by_year = tfidf_topk_by_year(
            texts_by_year=texts_by_year,
            tokenizer_func=tokenizer_func,
            min_df=args.min_df,
            max_df=args.max_df,
            ngram_range=(1, args.ngram_max),
            topk=args.topk
        )
    except Exception as e:
        print(f"âš ï¸ TF-IDF åˆ†æå¤±è´¥ï¼Œè·³è¿‡: {e}")
        tfidf_by_year = pd.DataFrame(columns=['year', 'word', 'score'])
    
    # ä¿å­˜ç»Ÿè®¡ç»“æœ
    print("ğŸ’¾ ä¿å­˜ç»Ÿè®¡ç»“æœ...")
    os.makedirs(args.output_dir, exist_ok=True)
    save_freq_stats(freq_overall, freq_by_year, tfidf_by_year, args.output_dir)
    
    # Zipf å®šå¾‹åˆ†æ
    print("ğŸ“ˆ ç”Ÿæˆ Zipf å®šå¾‹åˆ†æå›¾...")
    zipf_plot(
        freq_data=freq_overall,
        output_path=os.path.join(args.output_dir, "zipf_overall.png"),
        font_path=args.font_path
    )
    
    # ç”Ÿæˆè¯äº‘
    if args.make_wordcloud:
        print("ğŸ¨ ç”Ÿæˆè¯äº‘...")
        
        # æ•´ä½“è¯äº‘
        generate_overall_wordcloud(
            freq_data=freq_overall,
            output_dir=args.output_dir,
            mask_path="analysis/assets/mask.png",
            font_path=args.font_path,
            top_n=200
        )
        
        # å¹´åº¦è¯äº‘
        generate_yearly_wordclouds(
            freq_by_year=freq_by_year,
            output_dir=args.output_dir,
            mask_path="analysis/assets/mask.png", 
            font_path=args.font_path,
            top_n=100
        )
    
    # ç”ŸæˆæŠ¥å‘Š
    if args.report:
        print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        stats_summary = get_stats_summary(freq_overall, freq_by_year, tfidf_by_year)
        generate_report(
            stats_summary=stats_summary,
            freq_overall=freq_overall,
            freq_by_year=freq_by_year,
            tfidf_by_year=tfidf_by_year,
            output_dir=args.output_dir
        )
    
    print(f"ğŸ‰ åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    for file_name in os.listdir(args.output_dir):
        print(f"  - {file_name}")


if __name__ == "__main__":
    main()