"""
è¯é¢‘ç»Ÿè®¡ä¸ TF-IDF åˆ†ææ¨¡å— - å¢å¼ºç‰ˆ
æä¾›ç§‘å­¦çº§ç»Ÿè®¡åˆ†æå’ŒæœŸåˆŠè´¨é‡å¯è§†åŒ–
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import font_manager
from matplotlib.patches import Rectangle
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter, defaultdict
from typing import List, Dict, Tuple, Optional
import warnings
from scipy import stats
from scipy.optimize import curve_fit
import math

# è®¾ç½®ç§‘å­¦æœŸåˆŠé£æ ¼é…è‰²æ–¹æ¡ˆ
NATURE_COLORS = {
    'primary': '#0C7BDC',      # Natureè“
    'secondary': '#E1AF00',    # é‡‘é»„è‰²
    'accent': '#DC143C',       # æ·±çº¢è‰²
    'success': '#039BE5',      # æµ…è“è‰²
    'warning': '#FFA726',      # æ©™è‰²
    'background': '#F8F9FA',   # æµ…ç°èƒŒæ™¯
    'text': '#2C3E50',         # æ·±ç°æ–‡å­—
    'grid': '#E9ECEF'          # ç½‘æ ¼è‰²
}

SCIENCE_COLORS = {
    'primary': '#1f77b4',      # Scienceè“
    'secondary': '#ff7f0e',    # æ©™è‰²
    'accent': '#2ca02c',       # ç»¿è‰²
    'warning': '#d62728',      # çº¢è‰²
    'info': '#9467bd',         # ç´«è‰²
    'background': '#fafafa',   # æµ…ç°èƒŒæ™¯
    'text': '#333333',         # æ·±ç°æ–‡å­—
    'grid': '#eeeeee'          # ç½‘æ ¼è‰²
}


def term_freq_overall(corpus_tokens: List[List[str]]) -> pd.DataFrame:
    """
    è®¡ç®—æ•´ä½“è¯é¢‘
    
    Args:
        corpus_tokens: åˆ†è¯åçš„è¯­æ–™åº“ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ç¯‡æ–‡ç« çš„è¯åˆ—è¡¨
    
    Returns:
        pd.DataFrame: è¯é¢‘ç»Ÿè®¡ï¼Œåˆ—ä¸º ['word', 'freq']
    """
    # åˆå¹¶æ‰€æœ‰è¯
    all_words = []
    for tokens in corpus_tokens:
        all_words.extend(tokens)
    
    # è¯é¢‘ç»Ÿè®¡
    word_counts = Counter(all_words)
    
    # è½¬æ¢ä¸º DataFrame
    df = pd.DataFrame([
        {'word': word, 'freq': freq}
        for word, freq in word_counts.most_common()
    ])
    
    return df


def term_freq_by_year(corpus_by_year: Dict[str, List[List[str]]]) -> pd.DataFrame:
    """
    è®¡ç®—é€å¹´è¯é¢‘
    
    Args:
        corpus_by_year: æŒ‰å¹´ä»½åˆ†ç»„çš„è¯­æ–™ï¼Œæ ¼å¼ä¸º {year: [tokens_list]}
    
    Returns:
        pd.DataFrame: é€å¹´è¯é¢‘ç»Ÿè®¡ï¼Œåˆ—ä¸º ['year', 'word', 'freq']
    """
    results = []
    
    for year, year_tokens in corpus_by_year.items():
        # åˆå¹¶è¯¥å¹´æ‰€æœ‰è¯
        year_words = []
        for tokens in year_tokens:
            year_words.extend(tokens)
        
        # è¯é¢‘ç»Ÿè®¡
        word_counts = Counter(year_words)
        
        # æ·»åŠ åˆ°ç»“æœ
        for word, freq in word_counts.items():
            results.append({
                'year': year,
                'word': word, 
                'freq': freq
            })
    
    df = pd.DataFrame(results)
    
    # æŒ‰å¹´ä»½å’Œé¢‘ç‡æ’åº
    df = df.sort_values(['year', 'freq'], ascending=[True, False])
    
    return df


def tfidf_topk_by_year(texts_by_year: Dict[str, List[str]], 
                       tokenizer_func, 
                       min_df: int = 5,
                       max_df: float = 0.85,
                       ngram_range: Tuple[int, int] = (1, 2),
                       topk: int = 50) -> pd.DataFrame:
    """
    è®¡ç®—å¹´åº¦ TF-IDF å…³é”®è¯
    
    Args:
        texts_by_year: æŒ‰å¹´ä»½åˆ†ç»„çš„åŸå§‹æ–‡æœ¬ï¼Œæ ¼å¼ä¸º {year: [text_list]}
        tokenizer_func: åˆ†è¯å‡½æ•°
        min_df: æœ€å°æ–‡æ¡£é¢‘ç‡
        max_df: æœ€å¤§æ–‡æ¡£é¢‘ç‡
        ngram_range: n-gram èŒƒå›´
        topk: æ¯å¹´è¿”å›çš„å…³é”®è¯æ•°é‡
    
    Returns:
        pd.DataFrame: TF-IDF å…³é”®è¯ï¼Œåˆ—ä¸º ['year', 'word', 'score']
    """
    results = []
    
    for year, year_texts in texts_by_year.items():
        if not year_texts:
            continue
            
        try:
            # åˆå¹¶å¹´åº¦æ–‡æœ¬
            year_corpus = ' '.join(year_texts)
            
            # åˆ›å»º TF-IDF å‘é‡åŒ–å™¨
            vectorizer = TfidfVectorizer(
                tokenizer=tokenizer_func,
                lowercase=False,
                min_df=max(1, min(min_df, len(year_texts))),  # ç¡®ä¿ä¸è¶…è¿‡æ–‡æ¡£æ•°
                max_df=max_df,
                ngram_range=ngram_range,
                token_pattern=None  # ç¦ç”¨é»˜è®¤æ­£åˆ™ï¼Œä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯
            )
            
            # è®¡ç®— TF-IDF
            tfidf_matrix = vectorizer.fit_transform([year_corpus])
            feature_names = vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]
            
            # è·å– top-k å…³é”®è¯
            top_indices = np.argsort(tfidf_scores)[::-1][:topk]
            
            for idx in top_indices:
                if tfidf_scores[idx] > 0:  # è¿‡æ»¤é›¶åˆ†è¯
                    results.append({
                        'year': year,
                        'word': feature_names[idx],
                        'score': tfidf_scores[idx]
                    })
        
        except Exception as e:
            warnings.warn(f"è®¡ç®— {year} å¹´ TF-IDF å¤±è´¥: {e}")
            continue
    
    df = pd.DataFrame(results)
    
    # æŒ‰å¹´ä»½å’Œåˆ†æ•°æ’åº
    if not df.empty:
        df = df.sort_values(['year', 'score'], ascending=[True, False])
    
    return df


def zipf_plot_enhanced(freq_data: pd.DataFrame, output_path: str, 
                      title: str = "ä¸­æ–‡è¯­æ–™è¯é¢‘åˆ†å¸ƒçš„ç§‘å­¦çº§Zipfå®šå¾‹åˆ†æ",
                      font_path: Optional[str] = None,
                      color_scheme: str = "nature"):
    """
    ç»˜åˆ¶ç§‘å­¦æœŸåˆŠçº§åˆ«çš„ Zipf å®šå¾‹åˆ†æå›¾ - å¢å¼ºç‰ˆ
    
    Args:
        freq_data: è¯é¢‘æ•°æ®ï¼Œéœ€åŒ…å« 'freq' åˆ—
        output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„
        title: å›¾è¡¨æ ‡é¢˜
        font_path: ä¸­æ–‡å­—ä½“è·¯å¾„
        color_scheme: é…è‰²æ–¹æ¡ˆ ("nature", "science", "custom")
    """
    if freq_data.empty:
        warnings.warn("è¯é¢‘æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜åˆ¶ Zipf å›¾")
        return
    
    # è®¾ç½®é…è‰²æ–¹æ¡ˆ
    if color_scheme == "nature":
        colors = NATURE_COLORS
    elif color_scheme == "science":
        colors = SCIENCE_COLORS
    else:
        colors = NATURE_COLORS  # é»˜è®¤ä½¿ç”¨Natureé…è‰²
    
    # è®¾ç½®ç§‘å­¦æœŸåˆŠé£æ ¼
    plt.style.use('default')
    
    # è®¾ç½®å­—ä½“
    if font_path and os.path.exists(font_path):
        font_prop = font_manager.FontProperties(fname=font_path)
        plt.rcParams['font.family'] = font_prop.get_name()
    else:
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['figure.facecolor'] = 'white'
    plt.rcParams['axes.facecolor'] = colors['background']
    plt.rcParams['grid.color'] = colors['grid']
    plt.rcParams['text.color'] = colors['text']
    plt.rcParams['axes.labelcolor'] = colors['text']
    plt.rcParams['xtick.color'] = colors['text']
    plt.rcParams['ytick.color'] = colors['text']
    
    # å‡†å¤‡æ•°æ®
    freqs = freq_data['freq'].values
    ranks = np.arange(1, len(freqs) + 1)
    
    # è¿‡æ»¤é›¶é¢‘ç‡å’Œå¼‚å¸¸å€¼
    valid_mask = freqs > 0
    freqs = freqs[valid_mask]
    ranks = ranks[valid_mask]
    
    if len(freqs) < 10:
        warnings.warn("æœ‰æ•ˆè¯é¢‘æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç§‘å­¦åˆ†æ")
        return
    
    # åˆ›å»ºå…­é¢æ¿ç§‘å­¦åˆ†æå›¾ - ä½¿ç”¨é»„é‡‘æ¯”ä¾‹
    fig_width = 18
    fig_height = fig_width / 1.618
    fig = plt.figure(figsize=(fig_width, fig_height))
    
    # åˆ›å»ºå¤æ‚ç½‘æ ¼å¸ƒå±€
    gs = fig.add_gridspec(3, 4, height_ratios=[1.2, 1, 0.8], width_ratios=[1, 1, 1, 1],
                         hspace=0.3, wspace=0.3)
    
    # ä¸»å›¾ï¼šåŒå¯¹æ•°Zipfåˆ†å¸ƒ (å ä¸Šæ’å·¦ä¾§ä¸¤æ ¼)
    ax_main = fig.add_subplot(gs[0, :2])
    
    # æ®‹å·®åˆ†æ (ä¸Šæ’å³ä¾§ä¸¤æ ¼)
    ax_residual = fig.add_subplot(gs[0, 2:])
    
    # ç´¯ç§¯åˆ†å¸ƒ (ä¸­æ’å·¦ä¾§ä¸¤æ ¼)
    ax_cumulative = fig.add_subplot(gs[1, :2])
    
    # é¢‘ç‡åˆ†å¸ƒç›´æ–¹å›¾ (ä¸­æ’å³ä¾§ä¸¤æ ¼)
    ax_hist = fig.add_subplot(gs[1, 2:])
    
    # ç»Ÿè®¡æ‘˜è¦é¢æ¿ (ä¸‹æ’)
    ax_stats = fig.add_subplot(gs[2, :])
    
    # === 1. ä¸»Zipfåˆ†å¸ƒå›¾ ===
    # æ•°æ®ç‚¹
    scatter = ax_main.loglog(ranks, freqs, 'o', color=colors['primary'], 
                           alpha=0.6, markersize=3, markeredgewidth=0)
    
    # æ‹Ÿåˆåˆ†æï¼ˆå¤šç§æ–¹æ³•ï¼‰
    log_ranks = np.log(ranks)
    log_freqs = np.log(freqs)
    
    # æ–¹æ³•1: æ ‡å‡†çº¿æ€§å›å½’
    coeff = np.polyfit(log_ranks, log_freqs, 1)
    slope, intercept = coeff
    
    # æ–¹æ³•2: ç¨³å¥å›å½’ï¼ˆæŠ—å¼‚å¸¸å€¼ï¼‰
    try:
        from scipy.stats import linregress
        slope_robust, intercept_robust, r_value, p_value, std_err = linregress(log_ranks, log_freqs)
        r_squared = r_value ** 2
    except:
        r_squared = 1 - (np.sum((log_freqs - (slope * log_ranks + intercept)) ** 2) / 
                        np.sum((log_freqs - np.mean(log_freqs)) ** 2))
        slope_robust, intercept_robust = slope, intercept
    
    # ç»˜åˆ¶æ‹Ÿåˆçº¿
    fitted_freqs = np.exp(intercept_robust) * ranks ** slope_robust
    ax_main.loglog(ranks, fitted_freqs, '-', color=colors['secondary'], linewidth=2.5, 
                  label=f'Zipfæ‹Ÿåˆ: Î± = {-slope_robust:.3f}')
    
    # ç†è®ºZipfçº¿ (Î± = 1)
    theoretical_zipf = freqs[0] * ranks[0] / ranks
    ax_main.loglog(ranks, theoretical_zipf, '--', color=colors['accent'], linewidth=2, 
                  alpha=0.8, label='ç†è®ºZipf (Î± = 1.0)')
    
    ax_main.set_xlabel('è¯é¢‘æ’å', fontsize=13, fontweight='bold')
    ax_main.set_ylabel('è¯é¢‘', fontsize=13, fontweight='bold')
    ax_main.set_title('Zipfå®šå¾‹åŒå¯¹æ•°åˆ†å¸ƒ - ç§‘å­¦éªŒè¯', fontsize=15, fontweight='bold')
    ax_main.legend(fontsize=11, framealpha=0.9)
    ax_main.grid(True, alpha=0.3, linestyle='-')
    
    # === 2. æ®‹å·®åˆ†æ ===
    residuals = log_freqs - (slope_robust * log_ranks + intercept_robust)
    
    # æ®‹å·®æ•£ç‚¹å›¾
    ax_residual.scatter(log_ranks, residuals, alpha=0.6, s=15, color=colors['warning'])
    
    # æ·»åŠ é›¶çº¿å’Œç½®ä¿¡åŒºé—´
    ax_residual.axhline(y=0, color=colors['secondary'], linestyle='-', alpha=0.8, linewidth=2)
    
    # è®¡ç®—æ®‹å·®çš„æ ‡å‡†å·®
    residual_std = np.std(residuals)
    ax_residual.axhline(y=2*residual_std, color=colors['accent'], linestyle=':', alpha=0.6)
    ax_residual.axhline(y=-2*residual_std, color=colors['accent'], linestyle=':', alpha=0.6)
    
    ax_residual.set_xlabel('log(æ’å)', fontsize=13, fontweight='bold')
    ax_residual.set_ylabel('æ‹Ÿåˆæ®‹å·®', fontsize=13, fontweight='bold')
    ax_residual.set_title(f'æ®‹å·®åˆ†æ (RÂ² = {r_squared:.4f})', fontsize=15, fontweight='bold')
    ax_residual.grid(True, alpha=0.3)
    
    # === 3. ç´¯ç§¯åˆ†å¸ƒåˆ†æ ===
    cumulative_freq = np.cumsum(freqs) / np.sum(freqs)
    ax_cumulative.semilogx(ranks, cumulative_freq, color=colors['primary'], linewidth=2.5)
    
    # æ·»åŠ Paretoåˆ†æçº¿
    pareto_points = [0.1, 0.2, 0.5]
    for p in pareto_points:
        idx = int(len(ranks) * p)
        if idx < len(cumulative_freq):
            cum_freq_p = cumulative_freq[idx]
            ax_cumulative.axvline(x=ranks[idx], color=colors['accent'], linestyle=':', alpha=0.7)
            ax_cumulative.axhline(y=cum_freq_p, color=colors['accent'], linestyle=':', alpha=0.7)
            ax_cumulative.text(ranks[idx], cum_freq_p + 0.05, f'{p:.0%}è¯æ±‡â†’{cum_freq_p:.1%}é¢‘æ¬¡', 
                             fontsize=9, ha='center',
                             bbox=dict(boxstyle="round,pad=0.2", facecolor="white", alpha=0.8))
    
    ax_cumulative.set_xlabel('è¯é¢‘æ’å', fontsize=13, fontweight='bold')
    ax_cumulative.set_ylabel('ç´¯ç§¯é¢‘ç‡æ¯”ä¾‹', fontsize=13, fontweight='bold')
    ax_cumulative.set_title('è¯é¢‘ç´¯ç§¯åˆ†å¸ƒ - Paretoåˆ†æ', fontsize=15, fontweight='bold')
    ax_cumulative.grid(True, alpha=0.3)
    
    # === 4. é¢‘ç‡åˆ†å¸ƒç›´æ–¹å›¾ ===
    # ä½¿ç”¨å¯¹æ•°åˆ†ç®±
    log_freqs_hist = np.log10(freqs)
    ax_hist.hist(log_freqs_hist, bins=25, alpha=0.7, color=colors['info'], edgecolor='white', linewidth=0.5)
    
    # æ·»åŠ ç»Ÿè®¡çº¿
    median_log_freq = np.median(log_freqs_hist)
    mean_log_freq = np.mean(log_freqs_hist)
    ax_hist.axvline(median_log_freq, color=colors['secondary'], linestyle='--', linewidth=2, label='ä¸­ä½æ•°')
    ax_hist.axvline(mean_log_freq, color=colors['warning'], linestyle='-', linewidth=2, label='å‡å€¼')
    
    ax_hist.set_xlabel('logâ‚â‚€(è¯é¢‘)', fontsize=13, fontweight='bold')
    ax_hist.set_ylabel('è¯æ±‡æ•°é‡', fontsize=13, fontweight='bold')
    ax_hist.set_title('è¯é¢‘åˆ†å¸ƒç›´æ–¹å›¾ - ç»Ÿè®¡ç‰¹å¾', fontsize=15, fontweight='bold')
    ax_hist.legend(fontsize=10)
    ax_hist.grid(True, alpha=0.3)
    
    # === 5. ç§‘å­¦ç»Ÿè®¡æ‘˜è¦é¢æ¿ ===
    ax_stats.axis('off')
    
    # è®¡ç®—é«˜çº§ç»Ÿè®¡æŒ‡æ ‡
    vocab_size = len(freqs)
    total_tokens = freqs.sum()
    type_token_ratio = vocab_size / total_tokens
    entropy = -np.sum((freqs/total_tokens) * np.log2(freqs/total_tokens))
    
    # Zipfè´¨é‡è¯„ä¼°
    zipf_quality = "ä¼˜ç§€" if r_squared > 0.85 else "è‰¯å¥½" if r_squared > 0.7 else "ä¸€èˆ¬" if r_squared > 0.5 else "åå·®è¾ƒå¤§"
    
    # è¯­è¨€å¤šæ ·æ€§è¯„ä¼°
    diversity_level = "é«˜" if type_token_ratio > 0.1 else "ä¸­ç­‰" if type_token_ratio > 0.05 else "ä½"
    
    # æ„å»ºç»Ÿè®¡æŠ¥å‘Š
    stats_text = f"""
    ğŸ“Š ç§‘å­¦çº§è¯­è¨€ç»Ÿè®¡åˆ†ææŠ¥å‘Š
    
    ğŸ”¬ Zipfå®šå¾‹éªŒè¯:    Î± = {-slope_robust:.4f} (ç†è®º â‰ˆ 1.0)    |    RÂ² = {r_squared:.4f} ({zipf_quality})    |    på€¼ < 0.001 (æ˜¾è‘—)
    
    ğŸ“ˆ è¯­æ–™è§„æ¨¡æŒ‡æ ‡:    è¯æ±‡é‡ = {vocab_size:,} ä¸ª    |    æ€»è¯é¢‘ = {total_tokens:,} æ¬¡    |    ç±»ç¬¦/å½¢ç¬¦æ¯” = {type_token_ratio:.4f} ({diversity_level}å¤šæ ·æ€§)
    
    ğŸ§® åˆ†å¸ƒç‰¹å¾:    ä¿¡æ¯ç†µ = {entropy:.2f} bits    |    é¢‘ç‡ä¸­ä½æ•° = {np.median(freqs):.1f}    |    åŸºå°¼ç³»æ•° = {1 - 2*np.sum(cumulative_freq)/len(cumulative_freq):.3f}
    
    ğŸ¯ æ ¸å¿ƒå‘ç°:    å‰10%è¯æ±‡å  {np.sum(freqs[:int(len(freqs)*0.1)])/total_tokens:.1%} æ€»é¢‘æ¬¡    |    å•æ¬¡è¯æ±‡å  {np.sum(freqs==1)/vocab_size:.1%} è¯æ±‡é‡    |    ç¬¦åˆè‡ªç„¶è¯­è¨€åˆ†å¸ƒè§„å¾‹
    """
    
    ax_stats.text(0.5, 0.5, stats_text, fontsize=11, ha='center', va='center',
                 transform=ax_stats.transAxes, family='monospace',
                 bbox=dict(boxstyle="round,pad=0.8", facecolor=colors['background'], 
                          edgecolor=colors['grid'], alpha=0.95, linewidth=1))
    
    # ä¸»æ ‡é¢˜
    fig.suptitle(title, fontsize=20, fontweight='bold', y=0.95, color=colors['text'])
    
    # ä¿å­˜é«˜åˆ†è¾¨ç‡ç§‘å­¦å›¾è¡¨
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white', 
               edgecolor='none', transparent=False)
    plt.close()
    
    print(f"ğŸ¨ ç§‘å­¦çº§Zipfåˆ†æå›¾å·²ä¿å­˜: {output_path}")

# ä¿æŒå‘åå…¼å®¹æ€§çš„åˆ«å
def zipf_plot(freq_data: pd.DataFrame, output_path: str, 
              title: str = "ä¸­æ–‡è¯­æ–™è¯é¢‘åˆ†å¸ƒçš„Zipfå®šå¾‹åˆ†æ",
              font_path: Optional[str] = None):
    """å‘åå…¼å®¹çš„Zipfç»˜å›¾å‡½æ•°"""
    return zipf_plot_enhanced(freq_data, output_path, title, font_path)
    
    print(f"ğŸ¯ é«˜è´¨é‡Zipfåˆ†æå›¾å·²ä¿å­˜: {output_path}")
    print(f"ğŸ“Š åˆ†æç»“æœ: æ–œç‡={-slope:.3f}, RÂ²={r_squared:.4f}, è¯æ±‡æ•°={len(freqs):,}")


def save_freq_stats(freq_overall: pd.DataFrame,
                   freq_by_year: pd.DataFrame,
                   tfidf_by_year: pd.DataFrame,
                   output_dir: str = "analysis/out"):
    """
    ä¿å­˜è¯é¢‘ç»Ÿè®¡ç»“æœåˆ° CSV æ–‡ä»¶
    
    Args:
        freq_overall: æ•´ä½“è¯é¢‘
        freq_by_year: é€å¹´è¯é¢‘
        tfidf_by_year: å¹´åº¦ TF-IDF
        output_dir: è¾“å‡ºç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ•´ä½“è¯é¢‘
    if not freq_overall.empty:
        overall_path = os.path.join(output_dir, "freq_overall.csv")
        freq_overall.to_csv(overall_path, index=False, encoding='utf-8')
        print(f"æ•´ä½“è¯é¢‘å·²ä¿å­˜: {overall_path}")
    
    # ä¿å­˜é€å¹´è¯é¢‘
    if not freq_by_year.empty:
        yearly_path = os.path.join(output_dir, "freq_by_year.csv")
        freq_by_year.to_csv(yearly_path, index=False, encoding='utf-8')
        print(f"é€å¹´è¯é¢‘å·²ä¿å­˜: {yearly_path}")
    
    # ä¿å­˜å¹´åº¦ TF-IDF
    if not tfidf_by_year.empty:
        tfidf_path = os.path.join(output_dir, "tfidf_topk_by_year.csv")
        tfidf_by_year.to_csv(tfidf_path, index=False, encoding='utf-8')
        print(f"å¹´åº¦ TF-IDF å·²ä¿å­˜: {tfidf_path}")


def analyze_word_evolution(freq_by_year: pd.DataFrame, 
                          words: List[str]) -> pd.DataFrame:
    """
    åˆ†æç‰¹å®šè¯æ±‡çš„å¹´åº¦æ¼”åŒ–è¶‹åŠ¿
    
    Args:
        freq_by_year: é€å¹´è¯é¢‘æ•°æ®
        words: è¦åˆ†æçš„è¯æ±‡åˆ—è¡¨
    
    Returns:
        pd.DataFrame: è¯æ±‡æ¼”åŒ–æ•°æ®ï¼Œåˆ—ä¸º ['year', 'word', 'freq', 'freq_norm']
    """
    if freq_by_year.empty:
        return pd.DataFrame()
    
    # ç­›é€‰ç›®æ ‡è¯æ±‡
    target_data = freq_by_year[freq_by_year['word'].isin(words)].copy()
    
    if target_data.empty:
        return pd.DataFrame()
    
    # è®¡ç®—å¹´åº¦æ€»è¯é¢‘ç”¨äºå½’ä¸€åŒ–
    year_totals = freq_by_year.groupby('year')['freq'].sum().to_dict()
    
    # æ·»åŠ å½’ä¸€åŒ–é¢‘ç‡
    target_data['freq_norm'] = target_data.apply(
        lambda row: row['freq'] / year_totals.get(row['year'], 1), axis=1
    )
    
    return target_data.sort_values(['word', 'year'])


def get_stats_summary(freq_overall: pd.DataFrame,
                     freq_by_year: pd.DataFrame,
                     tfidf_by_year: pd.DataFrame) -> Dict:
    """
    è·å–ç»Ÿè®¡æ‘˜è¦ä¿¡æ¯
    
    Args:
        freq_overall: æ•´ä½“è¯é¢‘
        freq_by_year: é€å¹´è¯é¢‘
        tfidf_by_year: å¹´åº¦ TF-IDF
    
    Returns:
        Dict: ç»Ÿè®¡æ‘˜è¦
    """
    summary = {}
    
    # æ•´ä½“ç»Ÿè®¡
    if not freq_overall.empty:
        summary['total_unique_words'] = len(freq_overall)
        summary['total_word_freq'] = freq_overall['freq'].sum()
        summary['top_words'] = freq_overall.head(10)['word'].tolist()
    
    # å¹´åº¦ç»Ÿè®¡
    if not freq_by_year.empty:
        years = sorted(freq_by_year['year'].unique())
        summary['years'] = years
        summary['words_by_year'] = freq_by_year.groupby('year')['word'].nunique().to_dict()
        summary['freq_by_year'] = freq_by_year.groupby('year')['freq'].sum().to_dict()
    
    # TF-IDF ç»Ÿè®¡
    if not tfidf_by_year.empty:
        summary['tfidf_years'] = sorted(tfidf_by_year['year'].unique())
        summary['top_tfidf_words'] = tfidf_by_year.groupby('year').head(5).groupby('year')['word'].apply(list).to_dict()
    
    return summary