"""
è¯é¢‘ç»Ÿè®¡ä¸ TF-IDF åˆ†ææ¨¡å—
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import font_manager
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter, defaultdict
from typing import List, Dict, Tuple, Optional
import warnings


def term_freq_overall(corpus_tokens: List[List[str]]) -> pd.DataFrame:
    """
    è®¡ç®—æ•´ä½“è¯é¢‘
    
    Args:
        corpus_tokens: åˆ†è¯åçš„è¯­æ–™åº“ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ç¯‡æ–‡ç« çš„è¯åˆ—è¡¨
    
    Returns:
        pd.DataFrame: è¯é¢‘ç»Ÿè®¡ï¼Œåˆ—ä¸º ['word', 'freq']
    """
    # åˆå¹¶æ‰€æœ‰è¯
    all_words = []
    for tokens in corpus_tokens:
        all_words.extend(tokens)
    
    # è¯é¢‘ç»Ÿè®¡
    word_counts = Counter(all_words)
    
    # è½¬æ¢ä¸º DataFrame
    df = pd.DataFrame([
        {'word': word, 'freq': freq}
        for word, freq in word_counts.most_common()
    ])
    
    return df


def term_freq_by_year(corpus_by_year: Dict[str, List[List[str]]]) -> pd.DataFrame:
    """
    è®¡ç®—é€å¹´è¯é¢‘
    
    Args:
        corpus_by_year: æŒ‰å¹´ä»½åˆ†ç»„çš„è¯­æ–™ï¼Œæ ¼å¼ä¸º {year: [tokens_list]}
    
    Returns:
        pd.DataFrame: é€å¹´è¯é¢‘ç»Ÿè®¡ï¼Œåˆ—ä¸º ['year', 'word', 'freq']
    """
    results = []
    
    for year, year_tokens in corpus_by_year.items():
        # åˆå¹¶è¯¥å¹´æ‰€æœ‰è¯
        year_words = []
        for tokens in year_tokens:
            year_words.extend(tokens)
        
        # è¯é¢‘ç»Ÿè®¡
        word_counts = Counter(year_words)
        
        # æ·»åŠ åˆ°ç»“æœ
        for word, freq in word_counts.items():
            results.append({
                'year': year,
                'word': word, 
                'freq': freq
            })
    
    df = pd.DataFrame(results)
    
    # æŒ‰å¹´ä»½å’Œé¢‘ç‡æ’åº
    df = df.sort_values(['year', 'freq'], ascending=[True, False])
    
    return df


def tfidf_topk_by_year(texts_by_year: Dict[str, List[str]], 
                       tokenizer_func, 
                       min_df: int = 5,
                       max_df: float = 0.85,
                       ngram_range: Tuple[int, int] = (1, 2),
                       topk: int = 50) -> pd.DataFrame:
    """
    è®¡ç®—å¹´åº¦ TF-IDF å…³é”®è¯
    
    Args:
        texts_by_year: æŒ‰å¹´ä»½åˆ†ç»„çš„åŸå§‹æ–‡æœ¬ï¼Œæ ¼å¼ä¸º {year: [text_list]}
        tokenizer_func: åˆ†è¯å‡½æ•°
        min_df: æœ€å°æ–‡æ¡£é¢‘ç‡
        max_df: æœ€å¤§æ–‡æ¡£é¢‘ç‡
        ngram_range: n-gram èŒƒå›´
        topk: æ¯å¹´è¿”å›çš„å…³é”®è¯æ•°é‡
    
    Returns:
        pd.DataFrame: TF-IDF å…³é”®è¯ï¼Œåˆ—ä¸º ['year', 'word', 'score']
    """
    results = []
    
    for year, year_texts in texts_by_year.items():
        if not year_texts:
            continue
            
        try:
            # åˆå¹¶å¹´åº¦æ–‡æœ¬
            year_corpus = ' '.join(year_texts)
            
            # åˆ›å»º TF-IDF å‘é‡åŒ–å™¨
            vectorizer = TfidfVectorizer(
                tokenizer=tokenizer_func,
                lowercase=False,
                min_df=max(1, min(min_df, len(year_texts))),  # ç¡®ä¿ä¸è¶…è¿‡æ–‡æ¡£æ•°
                max_df=max_df,
                ngram_range=ngram_range,
                token_pattern=None  # ç¦ç”¨é»˜è®¤æ­£åˆ™ï¼Œä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯
            )
            
            # è®¡ç®— TF-IDF
            tfidf_matrix = vectorizer.fit_transform([year_corpus])
            feature_names = vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]
            
            # è·å– top-k å…³é”®è¯
            top_indices = np.argsort(tfidf_scores)[::-1][:topk]
            
            for idx in top_indices:
                if tfidf_scores[idx] > 0:  # è¿‡æ»¤é›¶åˆ†è¯
                    results.append({
                        'year': year,
                        'word': feature_names[idx],
                        'score': tfidf_scores[idx]
                    })
        
        except Exception as e:
            warnings.warn(f"è®¡ç®— {year} å¹´ TF-IDF å¤±è´¥: {e}")
            continue
    
    df = pd.DataFrame(results)
    
    # æŒ‰å¹´ä»½å’Œåˆ†æ•°æ’åº
    if not df.empty:
        df = df.sort_values(['year', 'score'], ascending=[True, False])
    
    return df


def zipf_plot(freq_data: pd.DataFrame, output_path: str, 
              title: str = "ä¸­æ–‡è¯­æ–™è¯é¢‘åˆ†å¸ƒçš„Zipfå®šå¾‹åˆ†æ",
              font_path: Optional[str] = None):
    """
    ç»˜åˆ¶é«˜è´¨é‡ç§‘å­¦ç ”ç©¶é£æ ¼çš„ Zipf å®šå¾‹åˆ†æå›¾
    
    Args:
        freq_data: è¯é¢‘æ•°æ®ï¼Œéœ€åŒ…å« 'freq' åˆ—
        output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„
        title: å›¾è¡¨æ ‡é¢˜
        font_path: ä¸­æ–‡å­—ä½“è·¯å¾„
    """
    if freq_data.empty:
        warnings.warn("è¯é¢‘æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜åˆ¶ Zipf å›¾")
        return
    
    # è®¾ç½®ç§‘å­¦ç ”ç©¶é£æ ¼
    plt.style.use('seaborn-v0_8-whitegrid' if 'seaborn-v0_8-whitegrid' in plt.style.available else 'default')
    
    # è®¾ç½®å­—ä½“
    if font_path and os.path.exists(font_path):
        font_prop = font_manager.FontProperties(fname=font_path)
        plt.rcParams['font.family'] = font_prop.get_name()
    else:
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    
    plt.rcParams['axes.unicode_minus'] = False
    
    # å‡†å¤‡æ•°æ®
    freqs = freq_data['freq'].values
    ranks = np.arange(1, len(freqs) + 1)
    
    # è¿‡æ»¤é›¶é¢‘ç‡
    valid_mask = freqs > 0
    freqs = freqs[valid_mask]
    ranks = ranks[valid_mask]
    
    if len(freqs) < 2:
        warnings.warn("æœ‰æ•ˆè¯é¢‘æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç»˜åˆ¶ Zipf å›¾")
        return
    
    # åˆ›å»ºå›¾è¡¨ - ä½¿ç”¨é»„é‡‘æ¯”ä¾‹
    fig_width = 16
    fig_height = fig_width / 1.618  # é»„é‡‘æ¯”ä¾‹
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(fig_width, fig_height))
    
    # ç§‘å­¦é…è‰²
    primary_color = '#2E86AB'
    secondary_color = '#A23B72' 
    accent_color = '#F18F01'
    
    # å·¦ä¸Šå›¾ï¼šåŒå¯¹æ•°æ•£ç‚¹å›¾
    ax1.loglog(ranks, freqs, 'o', color=primary_color, alpha=0.7, markersize=4)
    ax1.set_xlabel('è¯é¢‘æ’å', fontsize=12, fontweight='bold')
    ax1.set_ylabel('è¯é¢‘', fontsize=12, fontweight='bold')
    ax1.set_title('Zipfå®šå¾‹åŒå¯¹æ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold', color='#2C3E50')
    ax1.grid(True, alpha=0.3, linestyle='--')
    
    # æ‹Ÿåˆç›´çº¿
    log_ranks = np.log(ranks)
    log_freqs = np.log(freqs)
    
    # çº¿æ€§å›å½’
    coeff = np.polyfit(log_ranks, log_freqs, 1)
    slope, intercept = coeff
    
    # ç»˜åˆ¶æ‹Ÿåˆçº¿
    fitted_freqs = np.exp(intercept) * ranks ** slope
    ax1.loglog(ranks, fitted_freqs, '-', color=secondary_color, linewidth=3, 
               label=f'æ‹Ÿåˆçº¿: y = {np.exp(intercept):.1f} Ã— x^{slope:.3f}')
    ax1.legend(fontsize=11)
    
    # è®¡ç®— RÂ²
    ss_res = np.sum((log_freqs - (slope * log_ranks + intercept)) ** 2)
    ss_tot = np.sum((log_freqs - np.mean(log_freqs)) ** 2)
    r_squared = 1 - (ss_res / ss_tot)
    
    # å³ä¸Šå›¾ï¼šæ®‹å·®åˆ†æ
    residuals = log_freqs - (slope * log_ranks + intercept)
    ax2.scatter(log_ranks, residuals, alpha=0.6, s=20, color=accent_color)
    ax2.axhline(y=0, color=secondary_color, linestyle='--', alpha=0.8, linewidth=2)
    ax2.set_xlabel('log(æ’å)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('æ®‹å·®', fontsize=12, fontweight='bold')
    ax2.set_title(f'æ®‹å·®åˆ†æ (RÂ² = {r_squared:.4f})', fontsize=14, fontweight='bold', color='#2C3E50')
    ax2.grid(True, alpha=0.3, linestyle='--')
    
    # å·¦ä¸‹å›¾ï¼šç´¯ç§¯åˆ†å¸ƒ
    cumulative_freq = np.cumsum(freqs) / np.sum(freqs)
    ax3.semilogx(ranks, cumulative_freq, color=primary_color, linewidth=2)
    ax3.set_xlabel('è¯é¢‘æ’å', fontsize=12, fontweight='bold')
    ax3.set_ylabel('ç´¯ç§¯é¢‘ç‡æ¯”ä¾‹', fontsize=12, fontweight='bold')
    ax3.set_title('è¯é¢‘ç´¯ç§¯åˆ†å¸ƒ', fontsize=14, fontweight='bold', color='#2C3E50')
    ax3.grid(True, alpha=0.3, linestyle='--')
    
    # æ·»åŠ 80-20æ ‡è®°çº¿
    pareto_20_idx = int(len(ranks) * 0.2)
    pareto_freq_proportion = cumulative_freq[pareto_20_idx] if pareto_20_idx < len(cumulative_freq) else None
    if pareto_freq_proportion:
        ax3.axvline(x=ranks[pareto_20_idx], color=secondary_color, linestyle=':', alpha=0.8)
        ax3.axhline(y=pareto_freq_proportion, color=secondary_color, linestyle=':', alpha=0.8)
        ax3.text(ranks[pareto_20_idx], 0.1, f'å‰20%è¯æ±‡\nå {pareto_freq_proportion:.1%}é¢‘æ¬¡', 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
    
    # å³ä¸‹å›¾ï¼šè¯é¢‘åˆ†å¸ƒç›´æ–¹å›¾
    log_freq_bins = np.logspace(0, np.log10(freqs.max()), 30)
    ax4.hist(freqs, bins=log_freq_bins, alpha=0.7, color=accent_color, edgecolor='white')
    ax4.set_xscale('log')
    ax4.set_xlabel('è¯é¢‘', fontsize=12, fontweight='bold')
    ax4.set_ylabel('è¯æ±‡æ•°é‡', fontsize=12, fontweight='bold')
    ax4.set_title('è¯é¢‘åˆ†å¸ƒç›´æ–¹å›¾', fontsize=14, fontweight='bold', color='#2C3E50')
    ax4.grid(True, alpha=0.3, linestyle='--')
    
    # ä¸»æ ‡é¢˜
    fig.suptitle(title, fontsize=18, fontweight='bold', y=0.95, color='#2C3E50')
    
    # ç§‘å­¦åˆ†ææ€»ç»“
    # è®¡ç®—ä¸€äº›å…³é”®ç»Ÿè®¡é‡
    median_freq = np.median(freqs)
    freq_variance = np.var(freqs)
    top_10_percent_freq = np.sum(freqs[:int(len(freqs)*0.1)]) / np.sum(freqs)
    
    zipf_analysis = (
        f"ğŸ”¬ Zipfå®šå¾‹åˆ†æç»“æœ\n"
        f"â€¢ æ‹Ÿåˆæ–œç‡ Î± = {-slope:.3f} (ç†è®ºå€¼ â‰ˆ 1.0)\n"
        f"â€¢ æ‹Ÿåˆä¼˜åº¦ RÂ² = {r_squared:.4f}\n"
        f"â€¢ è¯æ±‡å¤šæ ·æ€§: {len(freqs):,} ä¸ªç‹¬ç‰¹è¯æ±‡\n"
        f"â€¢ æ€»è¯é¢‘: {freqs.sum():,} æ¬¡\n"
        f"â€¢ é¢‘ç‡ä¸­ä½æ•°: {median_freq:.1f}\n"
        f"â€¢ å‰10%è¯æ±‡å æ€»é¢‘æ¬¡: {top_10_percent_freq:.1%}\n"
        f"â€¢ è¯­è¨€åˆ†å¸ƒç¬¦åˆåº¦: {'ä¼˜ç§€' if r_squared > 0.8 else 'è‰¯å¥½' if r_squared > 0.6 else 'ä¸€èˆ¬'}"
    )
    
    fig.text(0.02, 0.02, zipf_analysis, fontsize=11, family='monospace',
             bbox=dict(boxstyle="round,pad=0.5", facecolor="#F8F9FA", 
                      edgecolor='#BDC3C7', alpha=0.95))
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.92, bottom=0.15)
    
    # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ğŸ¯ é«˜è´¨é‡Zipfåˆ†æå›¾å·²ä¿å­˜: {output_path}")
    print(f"ğŸ“Š åˆ†æç»“æœ: æ–œç‡={-slope:.3f}, RÂ²={r_squared:.4f}, è¯æ±‡æ•°={len(freqs):,}")


def save_freq_stats(freq_overall: pd.DataFrame,
                   freq_by_year: pd.DataFrame,
                   tfidf_by_year: pd.DataFrame,
                   output_dir: str = "analysis/out"):
    """
    ä¿å­˜è¯é¢‘ç»Ÿè®¡ç»“æœåˆ° CSV æ–‡ä»¶
    
    Args:
        freq_overall: æ•´ä½“è¯é¢‘
        freq_by_year: é€å¹´è¯é¢‘
        tfidf_by_year: å¹´åº¦ TF-IDF
        output_dir: è¾“å‡ºç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ•´ä½“è¯é¢‘
    if not freq_overall.empty:
        overall_path = os.path.join(output_dir, "freq_overall.csv")
        freq_overall.to_csv(overall_path, index=False, encoding='utf-8')
        print(f"æ•´ä½“è¯é¢‘å·²ä¿å­˜: {overall_path}")
    
    # ä¿å­˜é€å¹´è¯é¢‘
    if not freq_by_year.empty:
        yearly_path = os.path.join(output_dir, "freq_by_year.csv")
        freq_by_year.to_csv(yearly_path, index=False, encoding='utf-8')
        print(f"é€å¹´è¯é¢‘å·²ä¿å­˜: {yearly_path}")
    
    # ä¿å­˜å¹´åº¦ TF-IDF
    if not tfidf_by_year.empty:
        tfidf_path = os.path.join(output_dir, "tfidf_topk_by_year.csv")
        tfidf_by_year.to_csv(tfidf_path, index=False, encoding='utf-8')
        print(f"å¹´åº¦ TF-IDF å·²ä¿å­˜: {tfidf_path}")


def analyze_word_evolution(freq_by_year: pd.DataFrame, 
                          words: List[str]) -> pd.DataFrame:
    """
    åˆ†æç‰¹å®šè¯æ±‡çš„å¹´åº¦æ¼”åŒ–è¶‹åŠ¿
    
    Args:
        freq_by_year: é€å¹´è¯é¢‘æ•°æ®
        words: è¦åˆ†æçš„è¯æ±‡åˆ—è¡¨
    
    Returns:
        pd.DataFrame: è¯æ±‡æ¼”åŒ–æ•°æ®ï¼Œåˆ—ä¸º ['year', 'word', 'freq', 'freq_norm']
    """
    if freq_by_year.empty:
        return pd.DataFrame()
    
    # ç­›é€‰ç›®æ ‡è¯æ±‡
    target_data = freq_by_year[freq_by_year['word'].isin(words)].copy()
    
    if target_data.empty:
        return pd.DataFrame()
    
    # è®¡ç®—å¹´åº¦æ€»è¯é¢‘ç”¨äºå½’ä¸€åŒ–
    year_totals = freq_by_year.groupby('year')['freq'].sum().to_dict()
    
    # æ·»åŠ å½’ä¸€åŒ–é¢‘ç‡
    target_data['freq_norm'] = target_data.apply(
        lambda row: row['freq'] / year_totals.get(row['year'], 1), axis=1
    )
    
    return target_data.sort_values(['word', 'year'])


def get_stats_summary(freq_overall: pd.DataFrame,
                     freq_by_year: pd.DataFrame,
                     tfidf_by_year: pd.DataFrame) -> Dict:
    """
    è·å–ç»Ÿè®¡æ‘˜è¦ä¿¡æ¯
    
    Args:
        freq_overall: æ•´ä½“è¯é¢‘
        freq_by_year: é€å¹´è¯é¢‘
        tfidf_by_year: å¹´åº¦ TF-IDF
    
    Returns:
        Dict: ç»Ÿè®¡æ‘˜è¦
    """
    summary = {}
    
    # æ•´ä½“ç»Ÿè®¡
    if not freq_overall.empty:
        summary['total_unique_words'] = len(freq_overall)
        summary['total_word_freq'] = freq_overall['freq'].sum()
        summary['top_words'] = freq_overall.head(10)['word'].tolist()
    
    # å¹´åº¦ç»Ÿè®¡
    if not freq_by_year.empty:
        years = sorted(freq_by_year['year'].unique())
        summary['years'] = years
        summary['words_by_year'] = freq_by_year.groupby('year')['word'].nunique().to_dict()
        summary['freq_by_year'] = freq_by_year.groupby('year')['freq'].sum().to_dict()
    
    # TF-IDF ç»Ÿè®¡
    if not tfidf_by_year.empty:
        summary['tfidf_years'] = sorted(tfidf_by_year['year'].unique())
        summary['top_tfidf_words'] = tfidf_by_year.groupby('year').head(5).groupby('year')['word'].apply(list).to_dict()
    
    return summary