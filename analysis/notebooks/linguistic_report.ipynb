{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä¸­æ–‡è¯­æ–™åˆ†ææŠ¥å‘Š ğŸ“Š\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬ç”¨äºåˆ†æä¸ªäººå¾®ä¿¡å…¬ä¼—å·è¯­æ–™çš„è¯é¢‘ã€å…³é”®è¯å’Œå¯è§†åŒ–è¶‹åŠ¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®å›¾è¡¨æ ·å¼\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# æ·»åŠ æºä»£ç è·¯å¾„\n",
    "sys.path.append('../src')\n",
    "\n",
    "print(\"ğŸ“š ç¯å¢ƒè®¾ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ•°æ®åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–åˆ†æç»“æœ\n",
    "output_dir = '../out'\n",
    "\n",
    "# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "files_to_check = [\n",
    "    'freq_overall.csv',\n",
    "    'freq_by_year.csv', \n",
    "    'tfidf_topk_by_year.csv'\n",
    "]\n",
    "\n",
    "existing_files = []\n",
    "for file in files_to_check:\n",
    "    file_path = os.path.join(output_dir, file)\n",
    "    if os.path.exists(file_path):\n",
    "        existing_files.append(file)\n",
    "        print(f\"âœ… æ‰¾åˆ°æ–‡ä»¶: {file}\")\n",
    "    else:\n",
    "        print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file}\")\n",
    "\n",
    "if not existing_files:\n",
    "    print(\"\\nâš ï¸ è¯·å…ˆè¿è¡Œ CLI å·¥å…·ç”Ÿæˆåˆ†æç»“æœ\")\n",
    "    print(\"è¿è¡Œå‘½ä»¤: python -m analysis.src.cli --root Wechat-Backup/æ–‡ä¸åŠ ç‚¹çš„å¼ è¡”ç‘œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "freq_overall = pd.DataFrame()\n",
    "freq_by_year = pd.DataFrame()\n",
    "tfidf_by_year = pd.DataFrame()\n",
    "\n",
    "if 'freq_overall.csv' in existing_files:\n",
    "    freq_overall = pd.read_csv(os.path.join(output_dir, 'freq_overall.csv'))\n",
    "    print(f\"ğŸ“Š æ•´ä½“è¯é¢‘æ•°æ®: {len(freq_overall)} ä¸ªè¯æ±‡\")\n",
    "\n",
    "if 'freq_by_year.csv' in existing_files:\n",
    "    freq_by_year = pd.read_csv(os.path.join(output_dir, 'freq_by_year.csv'))\n",
    "    years = sorted(freq_by_year['year'].unique()) if not freq_by_year.empty else []\n",
    "    print(f\"ğŸ“… å¹´åº¦è¯é¢‘æ•°æ®: {len(years)} å¹´ï¼Œ{len(freq_by_year)} æ¡è®°å½•\")\n",
    "\n",
    "if 'tfidf_topk_by_year.csv' in existing_files:\n",
    "    tfidf_by_year = pd.read_csv(os.path.join(output_dir, 'tfidf_topk_by_year.csv'))\n",
    "    print(f\"ğŸ” TF-IDF æ•°æ®: {len(tfidf_by_year)} æ¡è®°å½•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¥ æ•´ä½“è¯é¢‘åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not freq_overall.empty:\n",
    "    # Top 20 è¯æ±‡æ¡å½¢å›¾\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_20 = freq_overall.head(20)\n",
    "    \n",
    "    bars = plt.barh(range(len(top_20)), top_20['freq'], \n",
    "                    color=plt.cm.viridis(np.linspace(0, 1, len(top_20))))\n",
    "    \n",
    "    plt.yticks(range(len(top_20)), top_20['word'])\n",
    "    plt.xlabel('è¯é¢‘')\n",
    "    plt.title('Top 20 é«˜é¢‘è¯æ±‡', fontsize=16, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + width*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{int(width):,}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ç»Ÿè®¡æ‘˜è¦\n",
    "    total_words = len(freq_overall)\n",
    "    total_freq = freq_overall['freq'].sum()\n",
    "    top_10_freq = freq_overall.head(10)['freq'].sum()\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ æ•´ä½“ç»Ÿè®¡æ‘˜è¦:\")\n",
    "    print(f\"æ€»è¯æ±‡æ•°: {total_words:,}\")\n",
    "    print(f\"æ€»è¯é¢‘: {total_freq:,}\")\n",
    "    print(f\"Top 10 è¯æ±‡å æ¯”: {top_10_freq/total_freq:.1%}\")\n",
    "    print(f\"è¯æ±‡å¤šæ ·æ€§ (è¯æ±‡æ•°/æ€»è¯é¢‘): {total_words/total_freq:.4f}\")\n",
    "else:\n",
    "    print(\"âŒ æ²¡æœ‰æ•´ä½“è¯é¢‘æ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“… å¹´åº¦è¶‹åŠ¿åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not freq_by_year.empty:\n",
    "    # å¹´åº¦è¯æ±‡é‡è¶‹åŠ¿\n",
    "    yearly_stats = freq_by_year.groupby('year').agg({\n",
    "        'word': 'nunique',  # è¯æ±‡æ•°\n",
    "        'freq': 'sum'       # æ€»è¯é¢‘\n",
    "    }).rename(columns={'word': 'unique_words', 'freq': 'total_freq'})\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # å¹´åº¦è¯æ±‡æ•°å˜åŒ–\n",
    "    ax1.plot(yearly_stats.index, yearly_stats['unique_words'], \n",
    "             marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
    "    ax1.set_title('å¹´åº¦è¯æ±‡æ•°å˜åŒ–', fontweight='bold')\n",
    "    ax1.set_xlabel('å¹´ä»½')\n",
    "    ax1.set_ylabel('è¯æ±‡æ•°')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å¹´åº¦æ€»è¯é¢‘å˜åŒ–\n",
    "    ax2.plot(yearly_stats.index, yearly_stats['total_freq'], \n",
    "             marker='s', linewidth=2, markersize=8, color='#A23B72')\n",
    "    ax2.set_title('å¹´åº¦æ€»è¯é¢‘å˜åŒ–', fontweight='bold')\n",
    "    ax2.set_xlabel('å¹´ä»½')\n",
    "    ax2.set_ylabel('æ€»è¯é¢‘')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ˜¾ç¤ºæ•°æ®è¡¨\n",
    "    yearly_stats['diversity'] = yearly_stats['unique_words'] / yearly_stats['total_freq']\n",
    "    print(\"\\nğŸ“Š å¹´åº¦ç»Ÿè®¡:\")\n",
    "    print(yearly_stats.round(4))\n",
    "else:\n",
    "    print(\"âŒ æ²¡æœ‰å¹´åº¦è¯é¢‘æ•°æ®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not freq_by_year.empty:\n",
    "    # é€‰æ‹©å‡ ä¸ªæœ‰è¶£çš„è¯æ±‡åˆ†æå…¶å¹´åº¦å˜åŒ–\n",
    "    # è‡ªåŠ¨é€‰æ‹©åœ¨å¤šä¸ªå¹´ä»½éƒ½å‡ºç°çš„é«˜é¢‘è¯\n",
    "    word_year_counts = freq_by_year.groupby('word')['year'].nunique().sort_values(ascending=False)\n",
    "    multi_year_words = word_year_counts[word_year_counts >= 3].index[:8]  # è‡³å°‘å‡ºç°3å¹´çš„å‰8ä¸ªè¯\n",
    "    \n",
    "    if len(multi_year_words) > 0:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(multi_year_words)))\n",
    "        \n",
    "        for i, word in enumerate(multi_year_words):\n",
    "            word_data = freq_by_year[freq_by_year['word'] == word]\n",
    "            plt.plot(word_data['year'], word_data['freq'], \n",
    "                    marker='o', label=word, linewidth=2, color=colors[i])\n",
    "        \n",
    "        plt.title('å…³é”®è¯æ±‡å¹´åº¦å˜åŒ–è¶‹åŠ¿', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('å¹´ä»½')\n",
    "        plt.ylabel('è¯é¢‘')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nğŸ” åˆ†æäº† {len(multi_year_words)} ä¸ªå¤šå¹´åº¦å‡ºç°çš„å…³é”®è¯\")\n",
    "    else:\n",
    "        print(\"âŒ æ²¡æœ‰æ‰¾åˆ°å¤šå¹´åº¦å‡ºç°çš„è¯æ±‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ TF-IDF å…³é”®è¯åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tfidf_by_year.empty:\n",
    "    # æ¯å¹´Top 10 TF-IDFå…³é”®è¯çƒ­åŠ›å›¾\n",
    "    years = sorted(tfidf_by_year['year'].unique())\n",
    "    \n",
    "    # è·å–æ¯å¹´å‰10ä¸ªå…³é”®è¯\n",
    "    top_words_by_year = {}\n",
    "    for year in years:\n",
    "        year_data = tfidf_by_year[tfidf_by_year['year'] == year].head(10)\n",
    "        top_words_by_year[year] = list(year_data['word'])\n",
    "    \n",
    "    # åˆ›å»ºè¯æ±‡-å¹´ä»½çŸ©é˜µ\n",
    "    all_top_words = set()\n",
    "    for words in top_words_by_year.values():\n",
    "        all_top_words.update(words)\n",
    "    \n",
    "    # æ„å»ºçƒ­åŠ›å›¾æ•°æ®\n",
    "    matrix_data = []\n",
    "    word_labels = []\n",
    "    \n",
    "    for word in all_top_words:\n",
    "        row = []\n",
    "        for year in years:\n",
    "            if word in top_words_by_year[year]:\n",
    "                # è·å–è¯¥è¯åœ¨è¯¥å¹´çš„æ’åï¼ˆå€’åºï¼Œæ’åè¶Šé«˜å€¼è¶Šå¤§ï¼‰\n",
    "                rank = top_words_by_year[year].index(word)\n",
    "                row.append(10 - rank)  # è½¬æ¢ä¸ºæ­£å‘æƒé‡\n",
    "            else:\n",
    "                row.append(0)\n",
    "        matrix_data.append(row)\n",
    "        word_labels.append(word)\n",
    "    \n",
    "    if matrix_data:\n",
    "        plt.figure(figsize=(12, min(20, len(word_labels))))\n",
    "        \n",
    "        # åªæ˜¾ç¤ºè‡³å°‘åœ¨2å¹´å‡ºç°çš„è¯æ±‡\n",
    "        filtered_data = []\n",
    "        filtered_labels = []\n",
    "        for i, row in enumerate(matrix_data):\n",
    "            if sum(1 for x in row if x > 0) >= 2:  # è‡³å°‘åœ¨2å¹´å‡ºç°\n",
    "                filtered_data.append(row)\n",
    "                filtered_labels.append(word_labels[i])\n",
    "        \n",
    "        if filtered_data:\n",
    "            sns.heatmap(filtered_data, \n",
    "                       xticklabels=years,\n",
    "                       yticklabels=filtered_labels,\n",
    "                       cmap='YlOrRd',\n",
    "                       annot=True,\n",
    "                       fmt='d',\n",
    "                       cbar_kws={'label': 'å…³é”®è¯é‡è¦æ€§ (10=æœ€é‡è¦)'})\n",
    "            \n",
    "            plt.title('å¹´åº¦ TF-IDF å…³é”®è¯çƒ­åŠ›å›¾', fontsize=16, fontweight='bold')\n",
    "            plt.xlabel('å¹´ä»½')\n",
    "            plt.ylabel('å…³é”®è¯')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"âŒ æ²¡æœ‰æ‰¾åˆ°å¤šå¹´åº¦çš„å…³é”®è¯\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæ¯å¹´çš„Top 5å…³é”®è¯\n",
    "    print(\"\\nğŸ¯ å„å¹´åº¦ Top 5 TF-IDF å…³é”®è¯:\")\n",
    "    for year in years:\n",
    "        year_data = tfidf_by_year[tfidf_by_year['year'] == year].head(5)\n",
    "        words = ', '.join(year_data['word'].tolist())\n",
    "        print(f\"{year}: {words}\")\n",
    "else:\n",
    "    print(\"âŒ æ²¡æœ‰ TF-IDF æ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¨ è¯äº‘å±•ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# æ˜¾ç¤ºæ•´ä½“è¯äº‘\n",
    "overall_wordcloud_path = os.path.join(output_dir, 'wordcloud_overall.png')\n",
    "if os.path.exists(overall_wordcloud_path):\n",
    "    print(\"ğŸ¨ æ•´ä½“è¯äº‘:\")\n",
    "    img = mpimg.imread(overall_wordcloud_path)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('æ•´ä½“è¯äº‘', fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âŒ æ•´ä½“è¯äº‘æ–‡ä»¶ä¸å­˜åœ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤ºå¹´åº¦è¯äº‘\n",
    "if not freq_by_year.empty:\n",
    "    years = sorted(freq_by_year['year'].unique())\n",
    "    \n",
    "    # è®¡ç®—å­å›¾å¸ƒå±€\n",
    "    n_years = len(years)\n",
    "    cols = min(3, n_years)\n",
    "    rows = (n_years + cols - 1) // cols\n",
    "    \n",
    "    if rows > 0:\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "        \n",
    "        if n_years == 1:\n",
    "            axes = [axes]\n",
    "        elif rows == 1:\n",
    "            axes = axes if hasattr(axes, '__len__') else [axes]\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        found_any = False\n",
    "        \n",
    "        for i, year in enumerate(years):\n",
    "            wordcloud_path = os.path.join(output_dir, f'wordcloud_{year}.png')\n",
    "            \n",
    "            if os.path.exists(wordcloud_path):\n",
    "                img = mpimg.imread(wordcloud_path)\n",
    "                axes[i].imshow(img)\n",
    "                axes[i].set_title(f'{year} å¹´è¯äº‘', fontweight='bold')\n",
    "                axes[i].axis('off')\n",
    "                found_any = True\n",
    "            else:\n",
    "                axes[i].text(0.5, 0.5, f'{year}\\nè¯äº‘æ–‡ä»¶\\nä¸å­˜åœ¨', \n",
    "                           ha='center', va='center', transform=axes[i].transAxes)\n",
    "                axes[i].axis('off')\n",
    "        \n",
    "        # éšè—å¤šä½™çš„å­å›¾\n",
    "        for i in range(n_years, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        if found_any:\n",
    "            plt.suptitle('å¹´åº¦è¯äº‘å¯¹æ¯”', fontsize=16, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "            print(\"âŒ æ²¡æœ‰æ‰¾åˆ°å¹´åº¦è¯äº‘æ–‡ä»¶\")\n",
    "else:\n",
    "    print(\"âŒ æ²¡æœ‰å¹´åº¦æ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’­ è‡ªæˆ‘é¼“åŠ±ä¸å¹´åº¦å£å·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆå¹´åº¦å£å·\n",
    "if not tfidf_by_year.empty:\n",
    "    print(\"ğŸ¯ åŸºäºå…³é”®è¯çš„å¹´åº¦å£å·:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # è‡ªæˆ‘é¼“åŠ±æ¨¡æ¿\n",
    "    templates = [\n",
    "        \"åœ¨{year}å¹´ï¼Œæˆ‘ä¸“æ³¨äº{word1}ï¼Œæ·±å…¥æ¢ç´¢{word2}ï¼ŒæŒç»­æ€è€ƒ{word3}ã€‚\",\n",
    "        \"{year}å¹´æ˜¯{word1}ä¹‹å¹´ï¼Œæˆ‘åœ¨{word2}é¢†åŸŸä¸æ–­æˆé•¿ï¼Œå¯¹{word3}æœ‰äº†æ›´æ·±çš„ç†è§£ã€‚\",\n",
    "        \"å›é¡¾{year}å¹´ï¼Œ{word1}æ˜¯æˆ‘çš„å…³é”®è¯ï¼Œé€šè¿‡{word2}å®ç°çªç ´ï¼Œåœ¨{word3}æ–¹é¢æ”¶è·æ»¡æ»¡ã€‚\",\n",
    "        \"{year}å¹´ï¼Œæˆ‘å°†{word1}ä½œä¸ºä¸»çº¿ï¼Œä»¥{word2}ä¸ºæ–¹æ³•ï¼Œå‘{word3}ç›®æ ‡å‰è¿›ã€‚\",\n",
    "        \"åœ¨{year}å¹´çš„æ—…ç¨‹ä¸­ï¼Œ{word1}æŒ‡å¼•æ–¹å‘ï¼Œ{word2}æä¾›åŠ¨åŠ›ï¼Œ{word3}å¸¦æ¥å¯å‘ã€‚\"\n",
    "    ]\n",
    "    \n",
    "    years = sorted(tfidf_by_year['year'].unique())\n",
    "    \n",
    "    for i, year in enumerate(years):\n",
    "        year_data = tfidf_by_year[tfidf_by_year['year'] == year].head(5)\n",
    "        if len(year_data) >= 3:\n",
    "            words = year_data['word'].tolist()\n",
    "            template = templates[i % len(templates)]\n",
    "            \n",
    "            slogan = template.format(\n",
    "                year=year,\n",
    "                word1=words[0],\n",
    "                word2=words[1] if len(words) > 1 else 'æ–°é¢†åŸŸ',\n",
    "                word3=words[2] if len(words) > 2 else 'äººç”Ÿå“²ç†'\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nğŸ“… {year}å¹´:\")\n",
    "            print(f\"   {slogan}\")\n",
    "            print(f\"   å…³é”®è¯: {', '.join(words[:5])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ’ª æ€»ç»“æ„Ÿè¨€:\")\n",
    "    \n",
    "    if not freq_overall.empty:\n",
    "        total_words = len(freq_overall)\n",
    "        total_freq = freq_overall['freq'].sum()\n",
    "        year_span = len(years)\n",
    "        \n",
    "        motivation = f\"\"\"é€šè¿‡ {year_span} å¹´çš„å†™ä½œå†ç¨‹ï¼Œæˆ‘ç§¯ç´¯äº† {total_words:,} ä¸ªä¸åŒçš„è¯æ±‡ï¼Œ\n",
    "æ€»è®¡ {total_freq:,} æ¬¡çš„è¡¨è¾¾ã€‚æ¯ä¸€ä¸ªè¯æ±‡éƒ½æ‰¿è½½ç€æ€è€ƒçš„ç—•è¿¹ï¼Œ\n",
    "æ¯ä¸€æ¬¡è¡¨è¾¾éƒ½è§è¯ç€æˆé•¿çš„è¶³è¿¹ã€‚\n",
    "\n",
    "ä»æ•°æ®ä¸­å¯ä»¥çœ‹å‡ºï¼Œæˆ‘çš„è¡¨è¾¾è¶Šæ¥è¶Šä¸°å¯Œï¼Œæ€è€ƒè¶Šæ¥è¶Šæ·±å…¥ã€‚\n",
    "è¿™ä¸ä»…æ˜¯æ–‡å­—çš„ç§¯ç´¯ï¼Œæ›´æ˜¯æ™ºæ…§çš„æ²‰æ·€ã€‚\n",
    "\n",
    "ç»§ç»­ä¿æŒè¿™ä»½çƒ­æƒ…ï¼Œè®©æ–‡å­—è®°å½•ç”Ÿæ´»ï¼Œè®©æ€è€ƒæŒ‡å¼•æœªæ¥ï¼ ğŸš€\"\"\"\n",
    "        \n",
    "        print(motivation)\n",
    "else:\n",
    "    print(\"âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”Ÿæˆå¹´åº¦å£å·\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Zipf å®šå¾‹åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤º Zipf åˆ†æå›¾\n",
    "zipf_path = os.path.join(output_dir, 'zipf_overall.png')\n",
    "if os.path.exists(zipf_path):\n",
    "    print(\"ğŸ“ˆ Zipf å®šå¾‹åˆ†æ:\")\n",
    "    img = mpimg.imread(zipf_path)\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Zipf å®šå¾‹åˆ†æ', fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“ Zipf å®šå¾‹è§£è¯»:\")\n",
    "    print(\"\"\"Zipf å®šå¾‹è¡¨æ˜ï¼Œè‡ªç„¶è¯­è¨€ä¸­è¯é¢‘ä¸å…¶æ’åå‘ˆåæ¯”å…³ç³»ã€‚\n",
    "è¿™åæ˜ äº†è¯­è¨€ä½¿ç”¨çš„ç»æµæ€§åŸåˆ™ï¼š\n",
    "- å°‘æ•°é«˜é¢‘è¯æ‰¿è½½ä¸»è¦ä¿¡æ¯ä¼ é€’åŠŸèƒ½\n",
    "- å¤§é‡ä½é¢‘è¯ä¸°å¯Œè¡¨è¾¾çš„ç²¾ç¡®æ€§å’Œä¸ªæ€§åŒ–\n",
    "- ç¬¦åˆ Zipf å®šå¾‹çš„è¯­è¨€ä½¿ç”¨ä½“ç°äº†è‡ªç„¶çš„å¹³è¡¡çŠ¶æ€\n",
    "\n",
    "ä»ä½ çš„å†™ä½œè¯­æ–™æ¥çœ‹ï¼Œè¿™ç§åˆ†å¸ƒç‰¹å¾è¡¨æ˜ä½ çš„æ–‡å­—è¡¨è¾¾æ—¢æœ‰æ ¸å¿ƒä¸»é¢˜ï¼Œ\n",
    "åˆå…·å¤‡ä¸°å¯Œçš„ç»†èŠ‚æè¿°ï¼Œä½“ç°äº†æˆç†Ÿçš„å†™ä½œé£æ ¼ã€‚\"\"\")\n",
    "else:\n",
    "    print(\"âŒ Zipf åˆ†æå›¾æ–‡ä»¶ä¸å­˜åœ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ æ•°æ®å¯¼å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªæ±‡æ€»çš„æ•°æ®é›†ç”¨äºè¿›ä¸€æ­¥åˆ†æ\n",
    "if not freq_overall.empty and not freq_by_year.empty:\n",
    "    # åˆå¹¶æ•°æ®åˆ›å»ºå®Œæ•´çš„åˆ†ææ•°æ®é›†\n",
    "    analysis_summary = {\n",
    "        'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_unique_words': len(freq_overall),\n",
    "        'total_word_freq': freq_overall['freq'].sum(),\n",
    "        'years_covered': sorted(freq_by_year['year'].unique()) if not freq_by_year.empty else [],\n",
    "        'top_10_words': freq_overall.head(10)['word'].tolist(),\n",
    "        'diversity_index': len(freq_overall) / freq_overall['freq'].sum()\n",
    "    }\n",
    "    \n",
    "    # ä¿å­˜æ±‡æ€»ä¿¡æ¯\n",
    "    import json\n",
    "    summary_path = os.path.join(output_dir, 'analysis_summary.json')\n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"ğŸ“Š åˆ†ææ±‡æ€»å·²ä¿å­˜åˆ°: {summary_path}\")\n",
    "    print(\"\\næ±‡æ€»ä¿¡æ¯:\")\n",
    "    for key, value in analysis_summary.items():\n",
    "        if key != 'top_10_words':\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(f\"  top_10_words: {', '.join(analysis_summary['top_10_words'])}\")\n",
    "else:\n",
    "    print(\"âŒ æ•°æ®ä¸å®Œæ•´ï¼Œæ— æ³•ç”Ÿæˆæ±‡æ€»\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ åˆ†æå®Œæˆ\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬å®Œæˆäº†å¯¹ä¸ªäººå¾®ä¿¡å…¬ä¼—å·è¯­æ–™çš„å…¨é¢åˆ†æï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **è¯é¢‘ç»Ÿè®¡** - äº†è§£æœ€å¸¸ç”¨çš„è¯æ±‡\n",
    "2. **å¹´åº¦è¶‹åŠ¿** - è§‚å¯Ÿå†™ä½œé£æ ¼çš„æ¼”å˜\n",
    "3. **å…³é”®è¯åˆ†æ** - è¯†åˆ«å„å¹´åº¦çš„æ ¸å¿ƒä¸»é¢˜\n",
    "4. **å¯è§†åŒ–å±•ç¤º** - ç›´è§‚å±•ç°æ•°æ®ç‰¹å¾\n",
    "5. **è‡ªæˆ‘æ¿€åŠ±** - åŸºäºæ•°æ®çš„æ­£å‘åé¦ˆ\n",
    "\n",
    "è¿™äº›åˆ†æä¸ä»…å±•ç°äº†å†™ä½œçš„æ•°é‡ç‰¹å¾ï¼Œæ›´é‡è¦çš„æ˜¯ä½“ç°äº†æ€ç»´çš„æ·±åº¦å’Œå¹¿åº¦ã€‚\n",
    "ç»§ç»­ä¿æŒè¿™ä»½å¯¹æ–‡å­—çš„çƒ­çˆ±å’Œå¯¹æ€è€ƒçš„åšæŒï¼ ğŸš€âœ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}