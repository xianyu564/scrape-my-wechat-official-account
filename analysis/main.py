#!/usr/bin/env python3
"""
ä¸­æ–‡è¯­æ–™åˆ†æä¸»ç¨‹åº - åˆ†ä¸¤æ­¥æ‰§è¡Œï¼šåˆ†æ + å‘ˆç°
"""

import os
import sys
import pickle
import warnings
from pathlib import Path
from typing import List, Optional, Dict, Any
import pandas as pd
from tqdm import tqdm
from datetime import datetime

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.io_utils import scan_articles, read_text, get_corpus_stats
from src.tokenize_zh import ChineseTokenizer
from src.freq_stats import (
    term_freq_overall, term_freq_by_year, tfidf_topk_by_year,
    zipf_plot, save_freq_stats, get_stats_summary
)
from src.wordcloud_viz import (
    generate_overall_wordcloud, generate_yearly_wordclouds,
    create_wordcloud_comparison
)


def analyze_corpus(
    root_dir: str,
    output_dir: str = "analysis/out",
    # æ—¶é—´è¿‡æ»¤é€‰é¡¹
    start_date: Optional[str] = None,  # æ ¼å¼: "2020-01-01"  
    end_date: Optional[str] = None,    # æ ¼å¼: "2023-12-31"
    years: Optional[List[str]] = None, # ä¾‹å¦‚: ["2021", "2022", "2023"]
    # åˆ†è¯å‚æ•°
    min_df: int = 5,                   # TF-IDFæœ€å°æ–‡æ¡£é¢‘ç‡
    max_df: float = 0.85,              # TF-IDFæœ€å¤§æ–‡æ¡£é¢‘ç‡  
    ngram_max: int = 2,                # æœ€å¤§n-gramé•¿åº¦
    topk: int = 50,                    # æ¯å¹´è¿”å›çš„å…³é”®è¯æ•°é‡
    # è‡ªå®šä¹‰æ–‡ä»¶è·¯å¾„
    userdict_path: Optional[str] = None,        # è‡ªå®šä¹‰è¯å…¸
    stopwords_path: str = "analysis/assets/stopwords_zh.txt", # åœç”¨è¯æ–‡ä»¶
    extra_stopwords_path: Optional[str] = None, # é¢å¤–åœç”¨è¯æ–‡ä»¶
) -> Dict[str, Any]:
    """
    ç¬¬ä¸€æ­¥ï¼šçº¯ç†è®ºåˆ†æ - å¤„ç†æ–‡æœ¬è¯­æ–™ï¼Œç”Ÿæˆç»Ÿè®¡æ•°æ®ï¼Œä¸äº§ç”Ÿå¯è§†åŒ–è¾“å‡º
    
    Args:
        root_dir: è¯­æ–™æ ¹ç›®å½•ï¼Œå¦‚ "Wechat-Backup/æ–‡ä¸åŠ ç‚¹çš„å¼ è¡”ç‘œ"
        output_dir: åˆ†æç»“æœè¾“å‡ºç›®å½•
        å…¶ä»–å‚æ•°è§æ³¨é‡Š
    
    Returns:
        åˆ†æç»“æœå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯
    """
    
    print("=" * 60)
    print("ğŸ”¬ ç¬¬ä¸€æ­¥ï¼šè¯­æ–™ç†è®ºåˆ†æ")
    print("=" * 60)
    
    # æ‰«ææ–‡ç« 
    print(f"ğŸ“ æ‰«æè¯­æ–™ç›®å½•: {root_dir}")
    articles = scan_articles(
        root_dir=root_dir,
        start_date=start_date,
        end_date=end_date,
        years=years
    )
    
    if not articles:
        raise ValueError("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œè¿‡æ»¤æ¡ä»¶")
    
    # æ˜¾ç¤ºè¯­æ–™ç»Ÿè®¡
    corpus_stats = get_corpus_stats(articles)
    print(f"ğŸ“Š è¯­æ–™ç»Ÿè®¡: {corpus_stats}")
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    print("ğŸ”¤ åˆå§‹åŒ–ä¸­æ–‡åˆ†è¯å™¨...")
    extra_stopwords = []
    if extra_stopwords_path and os.path.exists(extra_stopwords_path):
        try:
            with open(extra_stopwords_path, 'r', encoding='utf-8') as f:
                extra_stopwords = [line.strip() for line in f if line.strip()]
        except Exception as e:
            warnings.warn(f"åŠ è½½é¢å¤–åœç”¨è¯å¤±è´¥: {e}")
    
    tokenizer = ChineseTokenizer(
        userdict_path=userdict_path,
        stopwords_path=stopwords_path,
        extra_stopwords=extra_stopwords,
        cache_dir=os.path.join(output_dir, ".cache")
    )
    
    # è¯»å–å’Œåˆ†è¯æ–‡æœ¬
    print("ğŸ“– è¯»å–æ–‡ç« å†…å®¹å¹¶åˆ†è¯...")
    corpus_tokens = []
    corpus_by_year = {}
    texts_by_year = {}
    
    for article in tqdm(articles, desc="å¤„ç†æ–‡ç« "):
        text = read_text(article)
        if not text:
            continue
        
        tokens = tokenizer.tokenize(text, chinese_only=True, ngram_max=ngram_max)
        if tokens:
            corpus_tokens.append(tokens)
            
            # æŒ‰å¹´åˆ†ç»„
            year = article.year
            if year not in corpus_by_year:
                corpus_by_year[year] = []
                texts_by_year[year] = []
            
            corpus_by_year[year].append(tokens)
            texts_by_year[year].append(text)
    
    if not corpus_tokens:
        raise ValueError("âŒ æ²¡æœ‰æœ‰æ•ˆçš„åˆ†è¯ç»“æœ")
    
    print(f"âœ… æˆåŠŸå¤„ç† {len(corpus_tokens)} ç¯‡æ–‡ç« ")
    
    # è¯é¢‘ç»Ÿè®¡
    print("ğŸ“Š è®¡ç®—è¯é¢‘ç»Ÿè®¡...")
    freq_overall = term_freq_overall(corpus_tokens)
    freq_by_year = term_freq_by_year(corpus_by_year)
    
    # TF-IDF åˆ†æ
    print("ğŸ” è®¡ç®— TF-IDF å…³é”®è¯...")
    def create_tokenizer_func(tokenizer, chinese_only=True, ngram_max=1):
        def tokenize_func(text):
            return tokenizer.tokenize(text, chinese_only, ngram_max)
        return tokenize_func
    
    tokenizer_func = create_tokenizer_func(tokenizer, chinese_only=True, ngram_max=ngram_max)
    
    try:
        tfidf_by_year = tfidf_topk_by_year(
            texts_by_year=texts_by_year,
            tokenizer_func=tokenizer_func,
            min_df=min_df,
            max_df=max_df,
            ngram_range=(1, ngram_max),
            topk=topk
        )
    except Exception as e:
        print(f"âš ï¸ TF-IDF åˆ†æå¤±è´¥ï¼Œè·³è¿‡: {e}")
        tfidf_by_year = pd.DataFrame(columns=['year', 'word', 'score'])
    
    # ä¿å­˜åˆ†æç»“æœ
    print("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜ç»Ÿè®¡æ•°æ®
    save_freq_stats(freq_overall, freq_by_year, tfidf_by_year, output_dir)
    
    # ä¿å­˜ä¸­é—´ç»“æœåˆ°pickleæ–‡ä»¶ï¼Œä¾›ç¬¬äºŒæ­¥ä½¿ç”¨
    analysis_results = {
        'corpus_stats': corpus_stats,
        'articles': articles,
        'corpus_tokens': corpus_tokens,
        'corpus_by_year': corpus_by_year,
        'texts_by_year': texts_by_year,
        'freq_overall': freq_overall,
        'freq_by_year': freq_by_year,
        'tfidf_by_year': tfidf_by_year,
        'analysis_params': {
            'root_dir': root_dir,
            'start_date': start_date,
            'end_date': end_date,
            'years': years,
            'min_df': min_df,
            'max_df': max_df,
            'ngram_max': ngram_max,
            'topk': topk
        },
        'analysis_time': datetime.now().isoformat()
    }
    
    # ä¿å­˜åˆ†æç»“æœä¾›ç¬¬äºŒæ­¥ä½¿ç”¨
    results_path = os.path.join(output_dir, "analysis_results.pkl")
    with open(results_path, 'wb') as f:
        pickle.dump(analysis_results, f)
    
    print(f"ğŸ¯ ç†è®ºåˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“ˆ åˆ†ææ•°æ®å·²ä¿å­˜åˆ°: {results_path}")
    
    return analysis_results


def generate_visualizations(
    output_dir: str = "analysis/out",
    # è¯äº‘å‚æ•°
    make_wordcloud: bool = True,          # æ˜¯å¦ç”Ÿæˆè¯äº‘
    wordcloud_top_n: int = 200,           # æ•´ä½“è¯äº‘è¯æ±‡æ•°é‡
    yearly_wordcloud_top_n: int = 100,    # å¹´åº¦è¯äº‘è¯æ±‡æ•°é‡
    # æ–‡ä»¶è·¯å¾„
    font_path: Optional[str] = None,      # ä¸­æ–‡å­—ä½“æ–‡ä»¶è·¯å¾„
    mask_path: str = "analysis/assets/mask.png",  # è¯äº‘é®ç½©å›¾ç‰‡
    # æŠ¥å‘Šå‚æ•°  
    generate_report: bool = True,         # æ˜¯å¦ç”ŸæˆMarkdownæŠ¥å‘Š
    # Zipfåˆ†æ
    generate_zipf: bool = True,           # æ˜¯å¦ç”ŸæˆZipfå®šå¾‹åˆ†æå›¾
) -> Dict[str, str]:
    """
    ç¬¬äºŒæ­¥ï¼šå¯è§†åŒ–å‘ˆç° - åŸºäºç¬¬ä¸€æ­¥çš„åˆ†æç»“æœç”Ÿæˆè¯äº‘ã€å›¾è¡¨ã€æŠ¥å‘Šç­‰ç¾è§‚çš„å¯äº¤ä»˜æˆæœ
    
    Args:
        output_dir: è¾“å‡ºç›®å½•ï¼ˆåº”ä¸ç¬¬ä¸€æ­¥ç›¸åŒï¼‰
        å…¶ä»–å‚æ•°è§æ³¨é‡Š
    
    Returns:
        ç”Ÿæˆæ–‡ä»¶çš„è·¯å¾„å­—å…¸
    """
    
    print("=" * 60)  
    print("ğŸ¨ ç¬¬äºŒæ­¥ï¼šå¯è§†åŒ–å‘ˆç°")
    print("=" * 60)
    
    # åŠ è½½ç¬¬ä¸€æ­¥çš„åˆ†æç»“æœ
    results_path = os.path.join(output_dir, "analysis_results.pkl")
    if not os.path.exists(results_path):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶: {results_path}\nè¯·å…ˆè¿è¡Œç¬¬ä¸€æ­¥åˆ†æ")
    
    print("ğŸ“‚ åŠ è½½åˆ†æç»“æœ...")
    with open(results_path, 'rb') as f:
        analysis_results = pickle.load(f)
    
    freq_overall = analysis_results['freq_overall']
    freq_by_year = analysis_results['freq_by_year'] 
    tfidf_by_year = analysis_results['tfidf_by_year']
    corpus_stats = analysis_results['corpus_stats']
    
    generated_files = {}
    
    # Zipf å®šå¾‹åˆ†æ
    if generate_zipf:
        print("ğŸ“ˆ ç”Ÿæˆ Zipf å®šå¾‹åˆ†æå›¾...")
        zipf_path = os.path.join(output_dir, "zipf_overall.png")
        zipf_plot(
            freq_data=freq_overall,
            output_path=zipf_path,
            font_path=font_path
        )
        generated_files['zipf_plot'] = zipf_path
    
    # ç”Ÿæˆè¯äº‘
    if make_wordcloud:
        print("ğŸ¨ ç”Ÿæˆè¯äº‘...")
        
        # æ•´ä½“è¯äº‘
        overall_wordcloud_path = generate_overall_wordcloud(
            freq_data=freq_overall,
            output_dir=output_dir,
            mask_path=mask_path,
            font_path=font_path,
            top_n=wordcloud_top_n
        )
        if overall_wordcloud_path:
            generated_files['overall_wordcloud'] = overall_wordcloud_path
        
        # å¹´åº¦è¯äº‘
        yearly_wordcloud_paths = generate_yearly_wordclouds(
            freq_by_year=freq_by_year,
            output_dir=output_dir,
            mask_path=mask_path,
            font_path=font_path,
            top_n=yearly_wordcloud_top_n
        )
        generated_files['yearly_wordclouds'] = yearly_wordcloud_paths
    
    # ç”ŸæˆæŠ¥å‘Š
    if generate_report:
        print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        stats_summary = get_stats_summary(freq_overall, freq_by_year, tfidf_by_year)
        report_path = generate_markdown_report(
            stats_summary=stats_summary,
            freq_overall=freq_overall,
            freq_by_year=freq_by_year,
            tfidf_by_year=tfidf_by_year,
            corpus_stats=corpus_stats,
            analysis_params=analysis_results['analysis_params'],
            output_dir=output_dir
        )
        generated_files['report'] = report_path
    
    print(f"ğŸ‰ å¯è§†åŒ–å‘ˆç°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    for file_type, file_path in generated_files.items():
        if isinstance(file_path, list):
            print(f"  - {file_type}: {len(file_path)} ä¸ªæ–‡ä»¶")
            for path in file_path:
                print(f"    * {os.path.basename(path)}")
        else:
            print(f"  - {file_type}: {os.path.basename(file_path)}")
    
    return generated_files


def generate_markdown_report(
    stats_summary: dict,
    freq_overall: pd.DataFrame,
    freq_by_year: pd.DataFrame,
    tfidf_by_year: pd.DataFrame,
    corpus_stats: dict,
    analysis_params: dict,
    output_dir: str
) -> str:
    """ç”Ÿæˆ Markdown åˆ†ææŠ¥å‘Š"""
    
    report_path = os.path.join(output_dir, "REPORT.md")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ä¸­æ–‡è¯­æ–™åˆ†ææŠ¥å‘Š\n\n")
        
        # åˆ†æå‚æ•°
        f.write("## âš™ï¸ åˆ†æå‚æ•°\n\n")
        f.write(f"- **è¯­æ–™ç›®å½•**: {analysis_params['root_dir']}\n")
        if analysis_params['start_date']:
            f.write(f"- **å¼€å§‹æ—¥æœŸ**: {analysis_params['start_date']}\n")
        if analysis_params['end_date']:
            f.write(f"- **ç»“æŸæ—¥æœŸ**: {analysis_params['end_date']}\n")
        if analysis_params['years']:
            f.write(f"- **æŒ‡å®šå¹´ä»½**: {', '.join(analysis_params['years'])}\n")
        f.write(f"- **TF-IDF å‚æ•°**: min_df={analysis_params['min_df']}, max_df={analysis_params['max_df']}\n")
        f.write(f"- **N-gram é•¿åº¦**: {analysis_params['ngram_max']}\n")
        f.write(f"- **æ¯å¹´å…³é”®è¯æ•°**: {analysis_params['topk']}\n\n")
        
        # æ¦‚è§ˆç»Ÿè®¡
        f.write("## ğŸ“Š è¯­æ–™æ¦‚è§ˆ\n\n")
        if 'total_articles' in corpus_stats:
            f.write(f"- **æ€»æ–‡ç« æ•°**: {corpus_stats['total_articles']:,}\n")
        if 'total_unique_words' in stats_summary:
            f.write(f"- **æ€»è¯æ±‡æ•°**: {stats_summary['total_unique_words']:,}\n")
            f.write(f"- **æ€»è¯é¢‘**: {stats_summary['total_word_freq']:,}\n")
        
        if 'years' in stats_summary:
            f.write(f"- **å¹´ä»½èŒƒå›´**: {min(stats_summary['years'])} - {max(stats_summary['years'])}\n")
            f.write(f"- **è¦†ç›–å¹´æ•°**: {len(stats_summary['years'])} å¹´\n\n")
        
        # æ•´ä½“è¯é¢‘ Top 50
        f.write("## ğŸ”¥ æ•´ä½“é«˜é¢‘è¯æ±‡ (Top 50)\n\n")
        if not freq_overall.empty:
            top_50 = freq_overall.head(50)
            f.write("| æ’å | è¯æ±‡ | é¢‘æ¬¡ |\n")
            f.write("|------|------|------|\n")
            for i, row in top_50.iterrows():
                f.write(f"| {i+1} | {row['word']} | {row['freq']:,} |\n")
            f.write("\n")
        
        # æ•´ä½“è¯äº‘
        f.write("## ğŸ¨ æ•´ä½“è¯äº‘\n\n")
        f.write("![æ•´ä½“è¯äº‘](wordcloud_overall.png)\n\n")
        
        # å¹´åº¦åˆ†æ
        if 'years' in stats_summary:
            f.write("## ğŸ“… å¹´åº¦è¯é¢‘åˆ†æ\n\n")
            
            for year in sorted(stats_summary['years']):
                f.write(f"### {year} å¹´\n\n")
                
                # å¹´åº¦ Top 20 è¯é¢‘
                year_freq = freq_by_year[freq_by_year['year'] == year].head(20)
                if not year_freq.empty:
                    f.write("**é«˜é¢‘è¯æ±‡ (Top 20)**:\n\n")
                    f.write("| æ’å | è¯æ±‡ | é¢‘æ¬¡ |\n")
                    f.write("|------|------|------|\n")
                    for i, row in year_freq.iterrows():
                        rank = list(year_freq.index).index(i) + 1
                        f.write(f"| {rank} | {row['word']} | {row['freq']:,} |\n")
                    f.write("\n")
                
                # å¹´åº¦ TF-IDF Top 20  
                if not tfidf_by_year.empty and 'year' in tfidf_by_year.columns:
                    year_tfidf = tfidf_by_year[tfidf_by_year['year'] == year].head(20)
                    if not year_tfidf.empty:
                        f.write("**TF-IDF å…³é”®è¯ (Top 20)**:\n\n")
                        f.write("| æ’å | è¯æ±‡ | TF-IDF åˆ†æ•° |\n")
                        f.write("|------|------|-------------|\n")
                        for i, row in year_tfidf.iterrows():
                            rank = list(year_tfidf.index).index(i) + 1
                            f.write(f"| {rank} | {row['word']} | {row['score']:.4f} |\n")
                        f.write("\n")
                
                # å¹´åº¦è¯äº‘
                f.write(f"**{year} å¹´è¯äº‘**:\n\n")
                f.write(f"![{year}å¹´è¯äº‘](wordcloud_{year}.png)\n\n")
        
        # Zipf å®šå¾‹åˆ†æ
        f.write("## ğŸ“ˆ Zipf å®šå¾‹åˆ†æ\n\n")
        f.write("æ ¹æ® Zipf å®šå¾‹ï¼Œè¯é¢‘ä¸å…¶æ’åå‘ˆåæ¯”å…³ç³»ï¼ˆf âˆ 1/r^Î±ï¼‰ã€‚")
        f.write("ç†æƒ³æƒ…å†µä¸‹ï¼Œæ‹Ÿåˆæ–œç‡åº”æ¥è¿‘ -1ã€‚\n\n")
        f.write("![Zipfå®šå¾‹åˆ†æ](zipf_overall.png)\n\n")
        f.write("**è§£è¯»**: è¯­è¨€é£æ ¼ä½“ç°äº†å…¸å‹çš„é•¿å°¾åˆ†å¸ƒç‰¹å¾ï¼Œ")
        f.write("å°‘æ•°é«˜é¢‘è¯æ‰¿è½½ä¸»è¦è¯­è¨€ä¿¡æ¯ï¼Œå¤§é‡ä½é¢‘è¯ä¸°å¯Œè¡¨è¾¾çš„ç»†èŠ‚å’Œä¸ªæ€§ã€‚\n\n")
        
        # å¹´åº¦å£å·ç”Ÿæˆ
        if 'top_tfidf_words' in stats_summary and stats_summary['top_tfidf_words']:
            f.write("## ğŸ¯ å¹´åº¦å…³é”®è¯å£å·\n\n")
            for year, words in stats_summary['top_tfidf_words'].items():
                if words:
                    top_3 = words[:3]
                    slogan = f"åœ¨{year}å¹´ï¼Œæˆ‘ä¸“æ³¨äº{top_3[0]}ï¼Œæ·±å…¥æ¢ç´¢{top_3[1] if len(top_3) > 1 else 'æ–°é¢†åŸŸ'}ï¼ŒæŒç»­æ€è€ƒ{top_3[2] if len(top_3) > 2 else 'äººç”Ÿå“²ç†'}ã€‚"
                    f.write(f"**{year}**: {slogan}\n\n")
        
        # ç”Ÿæˆæ—¶é—´
        f.write(f"\n---\n\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
    
    print(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    return report_path


def main():
    """
    ä¸»å‡½æ•° - å¯ä»¥é€‰æ‹©è¿è¡Œåˆ†æã€å‘ˆç°æˆ–ä¸¤è€…
    """
    
    # =================================================================
    # ğŸ”§ é…ç½®å‚æ•° - æ ¹æ®éœ€è¦ä¿®æ”¹ä¸‹é¢çš„å‚æ•°
    # =================================================================
    
    # å¿…éœ€å‚æ•°
    ROOT_DIR = "Wechat-Backup/æ–‡ä¸åŠ ç‚¹çš„å¼ è¡”ç‘œ"  # è¯­æ–™æ ¹ç›®å½•
    OUTPUT_DIR = "analysis/out"                    # è¾“å‡ºç›®å½•
    
    # è¿è¡Œæ¨¡å¼é€‰æ‹©
    RUN_ANALYSIS = True       # æ˜¯å¦è¿è¡Œç¬¬ä¸€æ­¥ï¼ˆç†è®ºåˆ†æï¼‰
    RUN_VISUALIZATION = True  # æ˜¯å¦è¿è¡Œç¬¬äºŒæ­¥ï¼ˆå¯è§†åŒ–å‘ˆç°ï¼‰
    
    # ç¬¬ä¸€æ­¥ï¼šåˆ†æå‚æ•°
    ANALYSIS_PARAMS = {
        # æ—¶é—´è¿‡æ»¤
        'start_date': None,           # "2020-01-01" æˆ– None
        'end_date': None,             # "2023-12-31" æˆ– None  
        'years': None,                # ["2021", "2022", "2023"] æˆ– None
        
        # åˆ†è¯å‚æ•°
        'min_df': 5,                  # TF-IDFæœ€å°æ–‡æ¡£é¢‘ç‡
        'max_df': 0.85,               # TF-IDFæœ€å¤§æ–‡æ¡£é¢‘ç‡
        'ngram_max': 2,               # æœ€å¤§n-gramé•¿åº¦
        'topk': 50,                   # æ¯å¹´è¿”å›çš„å…³é”®è¯æ•°é‡
        
        # è‡ªå®šä¹‰æ–‡ä»¶è·¯å¾„  
        'userdict_path': None,        # è‡ªå®šä¹‰è¯å…¸æ–‡ä»¶
        'extra_stopwords_path': None, # é¢å¤–åœç”¨è¯æ–‡ä»¶
    }
    
    # ç¬¬äºŒæ­¥ï¼šå¯è§†åŒ–å‚æ•°
    VISUALIZATION_PARAMS = {
        # è¯äº‘è®¾ç½®
        'make_wordcloud': True,              # æ˜¯å¦ç”Ÿæˆè¯äº‘
        'wordcloud_top_n': 200,              # æ•´ä½“è¯äº‘è¯æ±‡æ•°é‡
        'yearly_wordcloud_top_n': 100,       # å¹´åº¦è¯äº‘è¯æ±‡æ•°é‡
        'font_path': None,                   # ä¸­æ–‡å­—ä½“æ–‡ä»¶è·¯å¾„ï¼Œå¦‚ "/path/to/simhei.ttf"
        'mask_path': "analysis/assets/mask.png",  # è¯äº‘é®ç½©å›¾ç‰‡
        
        # å…¶ä»–è¾“å‡º
        'generate_report': True,             # æ˜¯å¦ç”ŸæˆMarkdownæŠ¥å‘Š  
        'generate_zipf': True,               # æ˜¯å¦ç”ŸæˆZipfå®šå¾‹åˆ†æå›¾
    }
    
    # =================================================================
    # ğŸš€ æ‰§è¡Œåˆ†ææµç¨‹
    # =================================================================
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šç†è®ºåˆ†æ
        if RUN_ANALYSIS:
            analysis_results = analyze_corpus(
                root_dir=ROOT_DIR,
                output_dir=OUTPUT_DIR,
                **ANALYSIS_PARAMS
            )
            print("\n" + "="*60)
            print("âœ… ç¬¬ä¸€æ­¥å®Œæˆï¼å¯ä»¥ä¿®æ”¹å¯è§†åŒ–å‚æ•°åå•ç‹¬è¿è¡Œç¬¬äºŒæ­¥")
            print("="*60 + "\n")
        
        # ç¬¬äºŒæ­¥ï¼šå¯è§†åŒ–å‘ˆç°
        if RUN_VISUALIZATION:
            generated_files = generate_visualizations(
                output_dir=OUTPUT_DIR,
                **VISUALIZATION_PARAMS  
            )
            print("\n" + "="*60)
            print("ğŸ‰ å…¨éƒ¨åˆ†æå®Œæˆï¼")
            print("="*60 + "\n")
            
        # å¦‚æœåªè¿è¡Œä¸€æ­¥ï¼Œç»™å‡ºæç¤º
        if RUN_ANALYSIS and not RUN_VISUALIZATION:
            print("ğŸ’¡ æç¤ºï¼šè¦ç”Ÿæˆå¯è§†åŒ–ç»“æœï¼Œè¯·è®¾ç½® RUN_VISUALIZATION = True")
        elif RUN_VISUALIZATION and not RUN_ANALYSIS:
            print("ğŸ’¡ æç¤ºï¼šå¦‚éœ€é‡æ–°åˆ†æè¯­æ–™ï¼Œè¯·è®¾ç½® RUN_ANALYSIS = True")
            
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()