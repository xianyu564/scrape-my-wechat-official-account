# LoRA SFT Training Configuration
# QLoRA/LoRA 风格微调配置文件

# 基础模型配置
model_name: Qwen/Qwen2.5-7B-Instruct
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
bnb_4bit_use_double_quant: true

# LoRA 配置
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules: [q_proj, k_proj, v_proj, o_proj, up_proj, down_proj]

# 训练配置
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
max_steps: 1500          # 或 num_train_epochs: 1
logging_steps: 20
save_steps: 200
warmup_steps: 100
lr_scheduler_type: cosine

# 数据配置
dataset: data/sft_train.jsonl
eval_dataset: data/sft_val.jsonl
max_seq_length: 1024
packing: false

# 精度配置
bf16: true               # 支持则开；否则删
fp16: false              # 与bf16互斥

# 输出配置
output_dir: outputs/qwen25-7b-sft-lora
run_name: wechat-style-sft
hub_model_id: null       # 不上传到Hub
push_to_hub: false

# 评估配置
evaluation_strategy: steps
eval_steps: 200
save_strategy: steps
load_best_model_at_end: true
metric_for_best_model: eval_loss
greater_is_better: false

# 其他配置
dataloader_num_workers: 4
remove_unused_columns: false
optim: paged_adamw_32bit
gradient_checkpointing: true
logging_dir: logs
seed: 42

# 早停配置
early_stopping_patience: 3
early_stopping_threshold: 0.001