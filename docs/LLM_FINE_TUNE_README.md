# LLM é£æ ¼å¾®è°ƒä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

æœ¬é¡¹ç›®æä¾›äº†å®Œæ•´çš„ä¸ªäººå†™ä½œé£æ ¼å¾®è°ƒè§£å†³æ–¹æ¡ˆï¼ŒåŸºäº QLoRA/LoRA æŠ€æœ¯ï¼Œå®ç°åœ¨ä¸æ”¹åŠ¨åŸæœ‰ä»£ç åº“çš„å‰æä¸‹ï¼Œå¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–é£æ ¼é€‚é…ã€‚

## ğŸ¯ è®¾è®¡ç›®æ ‡

- **æœ€å°å…¥ä¾µ**: ä¸ä¿®æ”¹ä»»ä½•ç°æœ‰çš„æŠ“å–å™¨ä»£ç ï¼Œæ–°å¢åŠŸèƒ½ç‹¬ç«‹äºåŸç³»ç»Ÿ
- **éšç§ä¿æŠ¤**: æ‰€æœ‰è®­ç»ƒæ•°æ®å’Œæ¨¡å‹æƒé‡ä»…å­˜å‚¨åœ¨æœ¬åœ°ï¼Œä¸ä¸Šä¼ åˆ°ç¬¬ä¸‰æ–¹
- **å­¦æœ¯åˆè§„**: æä¾›å®Œæ•´çš„å¤ç°æ–‡æ¡£å’Œè¯„æµ‹æ–¹æ³•ï¼Œå¯ç›´æ¥ç”¨äºè®ºæ–‡å†™ä½œ
- **å³æ’å³ç”¨**: è®­ç»ƒåçš„ LoRA é€‚é…å™¨å¯å¿«é€ŸåŠ è½½/å¸è½½ï¼Œä¸å½±å“åŸºç¡€æ¨¡å‹

## ğŸ“ æ–‡ä»¶ç»“æ„

```
prep/style_sft_builder.py        # æ•°æ®é¢„å¤„ç†ï¼šä» Markdown + meta.json ç”Ÿæˆè®­ç»ƒæ•°æ®
train/lora_sft.yaml              # è®­ç»ƒé…ç½®ï¼šQLoRA/LoRA è¶…å‚æ•°è®¾ç½®
eval/style_eval.ipynb            # è¯„æµ‹å·¥å…·ï¼šå›°æƒ‘åº¦ã€é£æ ¼æŒ‡æ ‡ã€A/Bæµ‹è¯•
docs/LLM_FINE_TUNE_README.md     # æœ¬æ–‡æ¡£ï¼šä½¿ç”¨è¯´æ˜å’Œåˆè§„æŒ‡å—
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…å¿…è¦ä¾èµ–
pip install transformers peft trl bitsandbytes
pip install torch torchvision torchaudio
pip install datasets accelerate
pip install matplotlib seaborn jieba notebook
```

### 2. æ•°æ®å‡†å¤‡

```bash
# ä»å¾®ä¿¡å¤‡ä»½ç”Ÿæˆè®­ç»ƒæ•°æ®
python prep/style_sft_builder.py --backup_dir Wechat-Backup --output_dir data

# æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®
ls data/
# åº”è¯¥çœ‹åˆ°: sft_train.jsonl, sft_val.jsonl, dataset_stats.json
```

æ•°æ®å¤„ç†æµç¨‹ï¼š
- æ‰«æ `Wechat-Backup/**/` ä¸‹çš„ `.md` æ–‡ä»¶å’Œ `meta.json`
- æ¸…ç† Markdown æ ¼å¼ï¼ŒæŒ‰ 300-800 tokens åˆ‡å—
- ç”Ÿæˆä¸‰ç§è®­ç»ƒæ¨¡æ¿ï¼šæ”¹å†™ã€ç»­å†™ã€æ€»ç»“å±•å¼€
- è¾“å‡ºæ ‡å‡†çš„ JSONL æ ¼å¼è®­ç»ƒæ•°æ®

### 3. æ¨¡å‹è®­ç»ƒ

åˆ›å»ºç®€å•çš„è®­ç»ƒè„šæœ¬ `train_lora.py`ï¼š

```python
#!/usr/bin/env python3
"""
ä¸€é”®å¼ LoRA é£æ ¼å¾®è°ƒè„šæœ¬
"""
import yaml
import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig,
    TrainingArguments
)
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig, get_peft_model
import datasets

def main():
    # åŠ è½½é…ç½®
    with open("train/lora_sft.yaml", 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f)
    
    # é‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=cfg["load_in_4bit"],
        bnb_4bit_quant_type=cfg["bnb_4bit_quant_type"],
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(cfg["model_name"], use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        cfg["model_name"],
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16
    )
    
    # LoRAé…ç½®
    peft_config = LoraConfig(
        r=cfg["lora_r"],
        lora_alpha=cfg["lora_alpha"],
        lora_dropout=cfg["lora_dropout"],
        target_modules=cfg["target_modules"],
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    # åŠ è½½æ•°æ®é›†
    train_dataset = datasets.load_dataset("json", data_files=cfg["dataset"])["train"]
    eval_dataset = datasets.load_dataset("json", data_files=cfg["eval_dataset"])["train"]
    
    # æ ¼å¼åŒ–å‡½æ•°
    def format_prompts(examples):
        texts = []
        for system, input_text, output in zip(examples["system"], examples["input"], examples["output"]):
            text = f"<|system|>\n{system}\n<|user|>\n{input_text}\n<|assistant|>\n{output}<|end|>"
            texts.append(text)
        return {"text": texts}
    
    train_dataset = train_dataset.map(format_prompts, batched=True)
    eval_dataset = eval_dataset.map(format_prompts, batched=True)
    
    # è®­ç»ƒé…ç½®
    training_args = SFTConfig(
        output_dir=cfg["output_dir"],
        per_device_train_batch_size=cfg["per_device_train_batch_size"],
        gradient_accumulation_steps=cfg["gradient_accumulation_steps"],
        learning_rate=cfg["learning_rate"],
        max_steps=cfg["max_steps"],
        logging_steps=cfg["logging_steps"],
        save_steps=cfg["save_steps"],
        evaluation_strategy="steps",
        eval_steps=cfg.get("eval_steps", 200),
        bf16=cfg.get("bf16", True),
        gradient_checkpointing=True,
        optim="paged_adamw_32bit",
        lr_scheduler_type=cfg.get("lr_scheduler_type", "cosine"),
        warmup_steps=cfg.get("warmup_steps", 100),
        max_seq_length=cfg.get("max_seq_length", 1024),
        packing=False,
        dataset_text_field="text"
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model()
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {cfg['output_dir']}")

if __name__ == "__main__":
    main()
```

è¿è¡Œè®­ç»ƒï¼š

```bash
python train_lora.py
```

é¢„æœŸè®­ç»ƒæ—¶é—´ï¼š
- 7B æ¨¡å‹ + QLoRA: ~2-4å°æ—¶ (24GB GPU)
- 1.5B æ¨¡å‹: ~30-60åˆ†é’Ÿ (16GB GPU)

### 4. æ¨¡å‹è¯„æµ‹

æ‰“å¼€ Jupyter Notebook è¿›è¡Œè¯„æµ‹ï¼š

```bash
jupyter notebook eval/style_eval.ipynb
```

è¯„æµ‹åŒ…å«ä¸‰ä¸ªç»´åº¦ï¼š
1. **å›°æƒ‘åº¦ (PPL)**: åœ¨éªŒè¯é›†ä¸Šæ¯”è¾ƒåŸºåº§æ¨¡å‹ vs LoRA æ¨¡å‹
2. **é£æ ¼æŒ‡ç¤ºå™¨**: å­—æ•°åˆ†å¸ƒã€è¯é¢‘ã€åœç”¨è¯æ¯”ä¾‹ç­‰ç»Ÿè®¡ç‰¹å¾å¯¹æ¯”
3. **A/B äººå·¥è¯„æµ‹**: ç”Ÿæˆç›²è¯„æ ·æœ¬ï¼Œè¯„ä¼°"æ›´åƒä½œè€…"çš„ç¨‹åº¦

### 5. æ¨¡å‹æ¨ç†

åˆ›å»ºæ¨ç†è„šæœ¬è¿›è¡Œæµ‹è¯•ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import torch

# é…ç½®
base_model_name = "Qwen/Qwen2.5-7B-Instruct"
lora_path = "outputs/qwen25-7b-sft-lora"

# åŠ è½½æ¨¡å‹
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4")
tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    base_model_name, 
    quantization_config=bnb_config, 
    device_map="auto"
)
model = PeftModel.from_pretrained(model, lora_path)

# ç”Ÿæˆæ–‡æœ¬
prompt = "ç”¨æˆ‘çš„å£å»å†™ä¸€æ®µå…³äºæ—©æ™¨çš„æ„Ÿå—ï¼š"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs, 
    max_new_tokens=200, 
    temperature=0.7,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)
```

## ğŸ“Š è¯„æµ‹æ ‡å‡†

### å®šé‡æŒ‡æ ‡
- **å›°æƒ‘åº¦é™ä½**: LoRA æ¨¡å‹ç›¸å¯¹åŸºåº§æ¨¡å‹çš„ PPL æ”¹è¿›
- **é£æ ¼ä¸€è‡´æ€§**: å­—æ•°ã€å¥é•¿ã€è¯é¢‘åˆ†å¸ƒä¸åŸæ–‡çš„ç›¸ä¼¼åº¦
- **è¯æ±‡å¤šæ ·æ€§**: ç”Ÿæˆæ–‡æœ¬çš„è¯æ±‡ä¸°å¯Œç¨‹åº¦

### å®šæ€§è¯„ä¼°
- **A/B ç›²è¯„**: éšæœºæ ·æœ¬å¯¹æ¯”ï¼Œè¯„ä¼°"æ›´åƒä½œè€…"çš„èƒœç‡
- **è¯­ä¹‰è¿è´¯æ€§**: ç”Ÿæˆå†…å®¹çš„é€»è¾‘æ€§å’Œæµç•…åº¦
- **é£æ ¼ç‰¹å¾**: æ˜¯å¦ä¿æŒäº†åŸä½œè€…çš„è¯­è¨€ä¹ æƒ¯å’Œè¡¨è¾¾æ–¹å¼

## ğŸ”’ éšç§ä¸åˆè§„

### æ•°æ®ä¿æŠ¤
- **æœ¬åœ°å­˜å‚¨**: æ‰€æœ‰åŸå§‹è¯­æ–™ã€è®­ç»ƒæ•°æ®ã€æ¨¡å‹æƒé‡ä»…å­˜å‚¨åœ¨æœ¬åœ°
- **ä¸ä¸Šä¼ ç­–ç•¥**: ä¸¥ç¦å°†ä¸ªäººè¯­æ–™æˆ–é€‚é…å™¨æƒé‡ä¸Šä¼ åˆ°å…¬å…±å¹³å°
- **è®¿é—®æ§åˆ¶**: è®­ç»ƒç¯å¢ƒåº”è®¾ç½®é€‚å½“çš„è®¿é—®æƒé™æ§åˆ¶

### ä½¿ç”¨åˆè§„
- **ç‰ˆæƒå£°æ˜**: åŸå§‹æ–‡ç« å†…å®¹éµå¾ª CC BY-NC-SA 4.0 åè®®
- **ä»£ç å¼€æº**: å·¥å…·ä»£ç éµå¾ª MIT åè®®
- **å­¦æœ¯ä½¿ç”¨**: æœ¬é¡¹ç›®é€‚ç”¨äºä¸ªäººå­¦ä¹ å’Œå­¦æœ¯ç ”ç©¶ï¼Œå•†ä¸šä½¿ç”¨éœ€é¢å¤–æˆæƒ

### AI å·¥å…·æŠ«éœ²
- æœ¬é¡¹ç›®åœ¨å¼€å‘è¿‡ç¨‹ä¸­ä½¿ç”¨äº†ç”Ÿæˆå¼ AI å·¥å…·è¿›è¡Œä»£ç è„šæ‰‹æ¶æ­å»º
- æ‰€æœ‰ç”Ÿæˆå†…å®¹å‡ç»è¿‡äººå·¥å®¡æ ¸å’Œä¿®æ”¹
- AI å·¥å…·ä»…ä½œä¸ºè¾…åŠ©ï¼Œä¸äº«æœ‰ç½²åæƒ

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**
A: 
1. ä½¿ç”¨æ›´å°çš„åŸºç¡€æ¨¡å‹ï¼ˆå¦‚ Qwen2.5-1.5Bï¼‰
2. å‡å° batch_size å’Œ max_seq_length
3. å¯ç”¨ gradient_checkpointing
4. ä½¿ç”¨ 8-bit é‡åŒ–æ›¿ä»£ 4-bit

**Q: è®­ç»ƒæŸå¤±ä¸ä¸‹é™ï¼Ÿ**
A:
1. æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡å¤§/è¿‡å°
2. ç¡®è®¤æ•°æ®æ ¼å¼æ­£ç¡®
3. å¢åŠ  warmup_steps
4. æ£€æŸ¥ LoRA rank è®¾ç½®

**Q: ç”Ÿæˆæ•ˆæœä¸ç†æƒ³ï¼Ÿ**
A:
1. å¢åŠ è®­ç»ƒæ­¥æ•°æˆ–æ•°æ®é‡
2. è°ƒæ•´ LoRA å‚æ•° (r, alpha)
3. æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡
4. å°è¯•ä¸åŒçš„é‡‡æ ·ç­–ç•¥

### æ€§èƒ½ä¼˜åŒ–

- **è®­ç»ƒåŠ é€Ÿ**: ä½¿ç”¨ `flash-attention-2` åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—
- **å†…å­˜ä¼˜åŒ–**: å¯ç”¨ `gradient_checkpointing` å’Œ `dataloader_pin_memory`
- **å¹¶è¡Œè®­ç»ƒ**: å¤šå¡ç¯å¢ƒä¸‹ä½¿ç”¨ `accelerate` è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ

## ğŸ“– å­¦æœ¯åº”ç”¨

æœ¬é¡¹ç›®æä¾›çš„è¯„æµ‹æ–¹æ³•å’Œç»“æœå¯ç›´æ¥ç”¨äºä»¥ä¸‹å­¦æœ¯åœºæ™¯ï¼š

### è®ºæ–‡ç« èŠ‚
- **æ–¹æ³•è®º**: LoRA å¾®è°ƒæŠ€æœ¯å’Œæ•°æ®å¤„ç†æµç¨‹
- **å®éªŒè®¾è®¡**: å›°æƒ‘åº¦æµ‹è¯•ã€é£æ ¼æŒ‡æ ‡åˆ†æã€A/B è¯„æµ‹
- **ç»“æœåˆ†æ**: å®šé‡æŒ‡æ ‡å¯¹æ¯”å’Œå®šæ€§æ•ˆæœè¯„ä¼°

### å¯å¤ç°æ€§
- å®Œæ•´çš„é…ç½®æ–‡ä»¶å’Œè®­ç»ƒè„šæœ¬
- è¯¦ç»†çš„ç¯å¢ƒä¾èµ–å’Œç‰ˆæœ¬è¯´æ˜  
- æ ‡å‡†åŒ–çš„è¯„æµ‹æµç¨‹å’ŒæŒ‡æ ‡è®¡ç®—

### åˆè§„è¦æ±‚
- æ•°æ®ä½¿ç”¨å’Œéšç§ä¿æŠ¤è¯´æ˜
- AI å·¥å…·ä½¿ç”¨å£°æ˜
- å¼€æºåè®®å’Œç‰ˆæƒä¿¡æ¯

## ğŸ›£ï¸ æœªæ¥æ‰©å±•

### æŠ€æœ¯æ”¹è¿›
- æ”¯æŒæ›´å¤šåŸºç¡€æ¨¡å‹ï¼ˆLLaMA, ChatGLM, Baichuanï¼‰
- é›†æˆæ›´å…ˆè¿›çš„å¾®è°ƒæŠ€æœ¯ï¼ˆQLoRA, AdaLoRAï¼‰
- æ·»åŠ å¤šæ¨¡æ€èƒ½åŠ›ï¼ˆå›¾æ–‡æ··åˆç”Ÿæˆï¼‰

### åŠŸèƒ½å¢å¼º
- Web UI ç•Œé¢ç®€åŒ–ä½¿ç”¨æµç¨‹
- è‡ªåŠ¨åŒ–è¯„æµ‹ç®¡é“
- å¤šä½œè€…é£æ ¼å¯¹æ¯”åˆ†æ

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰æŠ€æœ¯é—®é¢˜æˆ–æ”¹è¿›å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- GitHub Issues: é¡¹ç›®é—®é¢˜è¿½è¸ª
- æŠ€æœ¯æ–‡æ¡£: `docs/` ç›®å½•ä¸‹çš„è¯¦ç»†è¯´æ˜
- ç¤¾åŒºè®¨è®º: åŠ å…¥ç›¸å…³æŠ€æœ¯äº¤æµç¾¤

---

**å…è´£å£°æ˜**: æœ¬å·¥å…·ä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œä¸ªäººå­¦ä¹ ä½¿ç”¨ã€‚ç”¨æˆ·åº”éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ï¼Œä¸å¾—å°†ç”Ÿæˆå†…å®¹ç”¨äºè™šå‡ä¿¡æ¯ä¼ æ’­æˆ–å…¶ä»–æœ‰å®³ç”¨é€”ã€‚